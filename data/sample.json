[
    {
      "title": "Decision Information Meets Large Language Models: The Future of Explainable Operations Research",
      "abstract": "Operations Research (OR) is vital for decision-making in many industries. While recent OR methods have seen significant improvements in automation and efficiency through integrating Large Language Models (LLMs), they still struggle to produce meaningful explanations. This lack of clarity raises concerns about transparency and trustworthiness in OR applications. To address these challenges, we propose a comprehensive framework, Explainable Operations Research (EOR), emphasizing actionable and understandable explanations accompanying optimization. The core of EOR is the concept of Decision Information, which emerges from what-if analysis and focuses on evaluating the impact of complex constraints (or parameters) changes on decision-making. Specifically, we utilize bipartite graphs to quantify the changes in the OR model and adopt LLMs to improve the explanation capabilities. Additionally, we introduce the first industrial benchmark to rigorously evaluate the effectiveness of explanations and analyses in OR, establishing a new standard for transparency and clarity in the field.",
      "citationCount": -1,
      "reviews": [
        {
          "review": "Although I am not deeply familiar with the OR domain, the integration of LLMs with OR problems seems both interesting and innovative. While I initially had concerns about the strict reasoning required for OR problems, it appears that these concerns are somehow addressed through the design choices and the experimental results presented in the paper. However, I may have missed some of the finer details of the concepts.\n\nOverall, the paper is well-presented and easy to follow. Some figures and diagrams could be simplified or made more intuitive, which would help readers with less background in LLMs or NLP better understand the concepts. In addition to the methods, the experimental setup is convincing and thorough.\n\nThe significance of this approach is clear. By using this framework, users can gain insights into critical aspects of OR problems that have often been overlooked, enhancing their understanding and decision-making.Since developing a benchmark is listed as one of the contributions, a more detailed introduction to this benchmark is expected. It would be helpful to expand on things like why certain problem categories were selected, how comprehensive the benchmark is across various OR tasks, and whether it reflects real-world complexity. Furthermore, examples showing how the benchmark effectively measures explainability in practice would strengthen the paper. This would provide more transparency about how the benchmark can be applied and generalized.\n\nAlthough the paper presents a dual evaluation (automated and expert) to assess explanation quality, it doesn’t fully explore how well these metrics align with user needs (who are, in other words, the major user groups if it is put into real-world use). For instance, it is unclear whether the explanation quality metrics are used to capture all the necessary aspects of understandability, particularly for non-experts. Providing more insights into how the explanations are judged, probably by including user feedback or more detailed breakdowns of the evaluation criteria, would offer a stronger case for the robustness of the framework.\n\nAn analysis of computation complexity/scalability would also help strengthen the paper. I have potential concerns regarding the computational complexity of the proposed framework, especially given the use of LLMs and graph-based methods.",
          "rating": 8
        },
        {
          "review": "The paper makes a focused contribution by addressing a niche problem: explainability in OR. While this fits within the broader category of explainability for LLMs, the specificity of this context is important. The presentation is for the most part clear (but see below), and the agentic workflow proposed could be practically useful for practitioners.Clarity and related work: The problem formulation would benefit from concrete examples to clarify what constitutes a \"problem\" and a \"query\" in the OR context. A more thorough description of OptiGuide would also help readers understand its role in the comparison. Bipartite graphs are mentioned multiple times, but it remains unclear why they are relevant or how they contribute to the framework—this needs clarification. The focus of EOR on coding tasks is not well explained, and it would be helpful to understand why this aspect is emphasized. Additionally, the paper proposes an agentic workflow but does not cite related literature; such citations should be included in the related work section.\n\nSimilarly, it’s also hard to judge the significance of the work because related work is hardly discussed. The workflow appears simple and I think that the novelty may not be enough for an ICLR paper. \n\nEvaluation results: This is the main weakness. The benchmark used in experiments is not publicly available and is poorly described, making it hard to assess the quality of the experiments. The experimental section mentions a \"proprietary LLM\" without specifying which model is used, adding ambiguity to the evaluation. The explanation quality assessment relies on LLMs, but it is unclear if any manual evaluation by the authors was performed to validate these judgments. The experimental section needs many more results; currently, it's hard for the reviewer to judge the quality of the work.",
          "rating": 5
        },
        {
          "review": "1. The paper proposes a novel use of LLMs for explainability in the field of Operations Research. The experiments show that such approach is effective as per the ratings given by LLMs as well as human OR experts.\n2. Example 3 is quite useful in understanding the task as well as the output of EOR.1. While the paper reads well, some parts are not very clear. In particular:\n - It is not clear how the decision information, the bi-partite graph, the edit distance computation etc. is connected to LLMs. What does `Since LLMs cannot directly perform this quantification, we utilize them to sense these processes and generate explanatory insights.` exactly translate to in figure 2?\n- The section 3.1 on problem formulation focusses on explanations. What is the approach/innovation for modeling that translates to improved modeling accuracy in Table 2?\n2. The dataset is not shared, the paper states `Notably, the question sets and the queries in this benchmark are developed from scratch and managed in-house`. Would it be made available publicly? Does `developed from scratch` mean manual creation?\n3. If EOR enhances the modeling process, could you compare modeling accuracy on other public benchmarks?",
          "rating": 6
        },
        {
          "review": "- The authors tackle a relevant real-world problem\n- The authors explore the intersection of explainability for operational research and LLMs - a hot topic right now- the authors compare their method against only one baseline\n - while one of the key contributions is the development of a benchmark dataset, it is not clear how the data was gathered or what tasks and metrics are considered for it. While some are mentioned, they are not described in detail. \n - the authors mention the concept of decision information as one of the pillars of explainable operational research. Nevertheless, it is unclear whether results were reported for it.\n - while the authors claim that their framework enables actionable and understandable explanations, they only assess the quality of explanations considering attribution and justification explanations, with no reference to evaluating whether and how actionable the explanations are.",
          "rating": 8
        }
      ],
      "meta_review": "After reading the reviewers' comments, and reviewing the paper, we recommend acceptance for Poster.\n\nBelow a detailed description of the paper, with key strengths and possibly remaining weaknesses.\n\nThe paper proposes a novel LLM-based explainable operations research framework that is evaluate on the a benchmark created by the authors. The experimental evaluation show that \ntheir framework outperforms two baselines in terms of accuracy and explanations. \n\nThe key strengths (S#) of the paper are as follows: \n\n- (S1)\tThe authors tackle a relevant real-world problem\n- (S2)\tThe paper proposes a novel use of LLMs for explainability in the field of Operations Research. The experiments show that such approach is effective as per the ratings given by LLMs as well as human OR experts.\n- (S3)\tThe paper makes a focused contribution by addressing a niche problem: explainability in OR. While this fits within the broader category of explainability for LLMs, the specificity of this context is important. The presentation is for the most part clear (but see below), and the agentic workflow proposed could be practically useful for practitioners.\n\nThe key weaknesses (W#) are as follows: \n\n- (W1)\tClarity and related work: The problem formulation would benefit from concrete examples to clarify what constitutes a \"problem\" and a \"query\" in the OR context. A more thorough description of OptiGuide would also help readers understand its role in the comparison. Bipartite graphs are mentioned multiple times, but it remains unclear why they are relevant or how they contribute to the framework—this needs clarification. The focus of EOR on coding tasks is not well explained, and it would be helpful to understand why this aspect is emphasized. Additionally, the paper proposes an agentic workflow but does not cite related literature; such citations should be included in the related work section.\n- (W2)\tEvaluation results: This is the main weakness. The benchmark used in experiments is not publicly available and is poorly described, making it hard to assess the quality of the experiments. The experimental section mentions a \"proprietary LLM\" without specifying which model is used, adding ambiguity to the evaluation. The explanation quality assessment relies on LLMs, but it is unclear if any manual evaluation by the authors was performed to validate these judgments. The experimental section needs many more results; currently, it's hard for the reviewer to judge the quality of the work.\n\nWe note that the authors addressed mostly all the comments from the reviewers.\nThe authors have been proactive in addressing the comments raised by the reviewers, and the reviewers were well engaged responding to the authors.\n\nWe agree with the reviewers comments, and recommendations, noting some of the weaknesses that we believe may remain are mentioned in the metareview.\n\nNo ethics review raised by the reviewers, and we agree with them.",
      "replies": [
        {
          "summary": "This paper proposes a framework called Explainable Operations Research (EOR) that combines Operation Research (OR) with LLMs. This idea aims to improve the clarity and trustworthiness of decision-making processes. The ultimately expected results should be clear, actionable explanations of how changes in constraints or parameters would affect OR solutions.\n\nThe claimed contributions in this paper are: (1) the authors formulate the problem of the explainable OR problems within the context of LLMs, (2) the authors introduce the concept of \"Decision Information\" and utilize bipartite graphs with LLMs to quantify the importance, (3) the authors develop a new benchmark specifically designed to evaluate the effectiveness.",
          "soundness": 3,
          "presentation": 3,
          "contribution": 3,
          "strengths": "Although I am not deeply familiar with the OR domain, the integration of LLMs with OR problems seems both interesting and innovative. While I initially had concerns about the strict reasoning required for OR problems, it appears that these concerns are somehow addressed through the design choices and the experimental results presented in the paper. However, I may have missed some of the finer details of the concepts.\n\nOverall, the paper is well-presented and easy to follow. Some figures and diagrams could be simplified or made more intuitive, which would help readers with less background in LLMs or NLP better understand the concepts. In addition to the methods, the experimental setup is convincing and thorough.\n\nThe significance of this approach is clear. By using this framework, users can gain insights into critical aspects of OR problems that have often been overlooked, enhancing their understanding and decision-making.",
          "weaknesses": "Since developing a benchmark is listed as one of the contributions, a more detailed introduction to this benchmark is expected. It would be helpful to expand on things like why certain problem categories were selected, how comprehensive the benchmark is across various OR tasks, and whether it reflects real-world complexity. Furthermore, examples showing how the benchmark effectively measures explainability in practice would strengthen the paper. This would provide more transparency about how the benchmark can be applied and generalized.\n\nAlthough the paper presents a dual evaluation (automated and expert) to assess explanation quality, it doesn’t fully explore how well these metrics align with user needs (who are, in other words, the major user groups if it is put into real-world use). For instance, it is unclear whether the explanation quality metrics are used to capture all the necessary aspects of understandability, particularly for non-experts. Providing more insights into how the explanations are judged, probably by including user feedback or more detailed breakdowns of the evaluation criteria, would offer a stronger case for the robustness of the framework.\n\nAn analysis of computation complexity/scalability would also help strengthen the paper. I have potential concerns regarding the computational complexity of the proposed framework, especially given the use of LLMs and graph-based methods.",
          "questions": "See some of the questions above in Weakness. In addition:\n\nRegarding the benchmark (for explainability): What were the criteria for selecting the problem categories, and how comprehensive is the benchmark in terms of covering different types of OR challenges? Additionally, have you validated the benchmark with industry professionals or in practical settings to ensure it reflects real-world complexity?\n\nRegarding the system-level workflow: Have you considered how decision-makers would interact with the system and its explanations in real time? What if the system makes mistakes, can it be corrected under human supervision?\n\nRegarding the experimental setups/scalability: The paper compares EOR to OptiGuide, but have you considered comparing your work to other explainability frameworks outside the OR domain? Can this work be somehow extended to other similar domains but with different contexts, e.g., automated reasoning?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 8,
          "confidence": 2,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "The work introduces a framework called Explainable Operations Research (EOR) that aims at enhancing transparency in Operations Research (OR) models that incorporate Large Language Models (LLMs). The authors argue that, despite the prevalence of LLMs in OR, the lack of explanations represents a problem. EOR addresses this gap by introducing the concept of “decision Information”. This workflow involves three LLM agents to generate answers to user queries and corresponding explanations. The authors show that their method outperforms other methods in terms of quality of answers and explanations provided.",
          "soundness": 2,
          "presentation": 2,
          "contribution": 2,
          "strengths": "The paper makes a focused contribution by addressing a niche problem: explainability in OR. While this fits within the broader category of explainability for LLMs, the specificity of this context is important. The presentation is for the most part clear (but see below), and the agentic workflow proposed could be practically useful for practitioners.",
          "weaknesses": "Clarity and related work: The problem formulation would benefit from concrete examples to clarify what constitutes a \"problem\" and a \"query\" in the OR context. A more thorough description of OptiGuide would also help readers understand its role in the comparison. Bipartite graphs are mentioned multiple times, but it remains unclear why they are relevant or how they contribute to the framework—this needs clarification. The focus of EOR on coding tasks is not well explained, and it would be helpful to understand why this aspect is emphasized. Additionally, the paper proposes an agentic workflow but does not cite related literature; such citations should be included in the related work section.\n\nSimilarly, it’s also hard to judge the significance of the work because related work is hardly discussed. The workflow appears simple and I think that the novelty may not be enough for an ICLR paper. \n\nEvaluation results: This is the main weakness. The benchmark used in experiments is not publicly available and is poorly described, making it hard to assess the quality of the experiments. The experimental section mentions a \"proprietary LLM\" without specifying which model is used, adding ambiguity to the evaluation. The explanation quality assessment relies on LLMs, but it is unclear if any manual evaluation by the authors was performed to validate these judgments. The experimental section needs many more results; currently, it's hard for the reviewer to judge the quality of the work.",
          "questions": "Posted as part of weaknesses.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 5,
          "confidence": 2,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "This paper proposes a LLM-based explainable operations research (EOR) framework and evaluates it on a benchmark they created. EOR is a multi-agent framework that starts with a user query and generates an OR program (for Gurobi optimization solver), uses the program to solve the problem and generates explanations of the code as well as answers using LLMs. The experimental evaluation of accuracy and explanations shows that EOR outperforms two baselines.",
          "soundness": 3,
          "presentation": 2,
          "contribution": 3,
          "strengths": "1. The paper proposes a novel use of LLMs for explainability in the field of Operations Research. The experiments show that such approach is effective as per the ratings given by LLMs as well as human OR experts.\n2. Example 3 is quite useful in understanding the task as well as the output of EOR.",
          "weaknesses": "1. While the paper reads well, some parts are not very clear. In particular:\n - It is not clear how the decision information, the bi-partite graph, the edit distance computation etc. is connected to LLMs. What does `Since LLMs cannot directly perform this quantification, we utilize them to sense these processes and generate explanatory insights.` exactly translate to in figure 2?\n- The section 3.1 on problem formulation focusses on explanations. What is the approach/innovation for modeling that translates to improved modeling accuracy in Table 2?\n2. The dataset is not shared, the paper states `Notably, the question sets and the queries in this benchmark are developed from scratch and managed in-house`. Would it be made available publicly? Does `developed from scratch` mean manual creation?\n3. If EOR enhances the modeling process, could you compare modeling accuracy on other public benchmarks?",
          "questions": "1. Are the ground truth expert ratings on the quality of explanations part of the benchmark?\n2. Why is Table 3 missing some cells?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 6,
          "confidence": 3,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "The authors introduce a framework emphasizing actionable and understandable operational research explanations. They explore how constraints affect decision-making and introduce an industrial benchmark to assess the effectiveness of explanations in operational research.",
          "soundness": 3,
          "presentation": 3,
          "contribution": 3,
          "strengths": "- The authors tackle a relevant real-world problem\n- The authors explore the intersection of explainability for operational research and LLMs - a hot topic right now",
          "weaknesses": "- the authors compare their method against only one baseline\n - while one of the key contributions is the development of a benchmark dataset, it is not clear how the data was gathered or what tasks and metrics are considered for it. While some are mentioned, they are not described in detail. \n - the authors mention the concept of decision information as one of the pillars of explainable operational research. Nevertheless, it is unclear whether results were reported for it.\n - while the authors claim that their framework enables actionable and understandable explanations, they only assess the quality of explanations considering attribution and justification explanations, with no reference to evaluating whether and how actionable the explanations are.",
          "questions": "Dear authors, \n\nWe consider the research relevant and interesting. Nevertheless, we would like to point to some improvement opportunities:\n\nGENERAL COMMENTS\n\n(1) - We encourage the authors to strengthen the related work considering works related to explainability for operations research. E.g., (a) De Bock, Koen W., et al. \"Explainable AI for operational research: A defining framework, methods, applications, and a research agenda.\" European Journal of Operational Research 317.2 (2024): 249-272. and (b) Thuy, Arthur, and Dries F. Benoit. \"Explainability through uncertainty: Trustworthy decision-making with neural networks.\" European Journal of Operational Research 317.2 (2024): 330-340. Furthermore, the authors could briefly reference explainability metrics mentioned in the literature relevant to this work.\n\n(2) - The authors compare the proposed method only against the OptiGuide baseline. Are there any additional baseline methods they could compare to?\n\n(3) - Do the results reported for Auto vs. Expert only consider the cases when the LLM did not display any kind of errors? Do the scores degrade if considering cases with errors vs. the ones displayed by the baseline model?\n\n(4) - In Section 4.7, the authors provide detailed insight into the failure cases for the proposed method. What are the failure ratios for the baseline method and error typification? Is the proposed method more reliable in this regard? Could the authors increase the number of executions? For example, having two failures in 14 executions is different from observing the same proportion in 100 runs. \n\n(5) - One of the key contributions of the paper is the creation of a new industrial benchmark dataset specifically tailored to assess the effectiveness of explanations in OR. We would invite the authors to introduce it in greater detail: (a) how was the data obtained?, (b) what tasks were considered and why?, (c) how were the queries created/what experience informed the queries creation and how it was validated that they capture a wide range of scenarios?, (d) what metrics are considered in the benchmark and what is their underlying rationale?, (e) are there any other benchmarks in the field or close domains that should be considered (e.g., to draw inspiration)?, (f) is the benchmark published/will be published?\n\n(6)- The authors mention two aspects being measured to assess the quality of explanations: attribution and justification. We would appreciate some examples of how such explanations look and how they are assessed for correctness. \n\n(7)—One of the framework's key contributions is the generation of actionable explanations. Nevertheless, we miss a thorough evaluation of this aspect in the explanations generated. Furthermore, it is unclear to us whether this aspect was also included in the proposed benchmark.\n\n(8) - One of the paper's key contributions is the concept of Decision Information and its quantification. Nevertheless, no results have been reported on it. Do the authors consider that reporting it could enhance the understanding of their research work and provide an additional perspective to the ones already exposed in the manuscript?\n\nFIGURES\n\n(9) - Figure 3: We encourage the authors to provide a brief explanation of what the green/yellow highlighted explanations mean. Do they refer to attribution and justification explanation aspects?\n\nTABLES\n\n(10) - Table 2: in the caption, please indicate what does bolding the results mean\n\n(11) - Table 3: highlight the best results (e.g., by bolding them)\n\n(12) - All tables reporting metrics: provide arrows next to the metric names (up/down) indicating whether a higher/lower result is better.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 8,
          "confidence": 4,
          "code_of_conduct": "Yes"
        }
      ],
      "contribution": "1) We formulate the problem of the explainable OR problem within the context of LLMs, laying a foundation for future research in this area. 2) We introduce the concept of 'Decision Information' and utilize bipartite graphs in conjunction with LLMs to quantify its importance in response to user queries, enhancing both the modeling capabilities and the explanation of complex what-if analysis within OR. 3) We develop a new benchmark specifically designed to evaluate the effectiveness of explanations in OR, setting a new standard for explainability in the field.",
      "paper_id": "W2dR6rypBQ",
      "pdf": "/pdf/35945c678cce19346af59cd1d2742777d5f3c75d.pdf",
      "venue": "ICLR 2025 Poster",
      "summary": {
        "introduction": "Operations Research (OR) is a critical field used for decision-making across various industries such as logistics, finance, healthcare, and transportation. The recent integration of Large Language Models (LLMs) into OR has improved automation and efficiency, especially in generating modeling code and solutions. However, these approaches predominantly focus on solution accuracy and efficiency, while providing limited explanations for their decisions, raising concerns about transparency and trustworthiness. The introduction highlights the importance of explainability in OR, noting legal and ethical considerations like the GDPR’s 'right to explanation'. The limitations of current explainability methods are discussed, including their superficial nature, inability to handle complex scenario changes dynamically, and lack of quantitative analysis, which impairs understanding of how decisions are derived. To address these challenges, the authors propose a new comprehensive framework, Explainable Operations Research (EOR), which emphasizes actionable and understandable explanations—particularly around the concept of 'Decision Information'—and leverages bipartite graphs alongside LLMs to quantify decision-impact changes, especially when complex constraint modifications occur. The introduction also mentions the development of a new industrial benchmark for evaluating the quality of explanations, setting a new standard for transparency in OR.",
        "related_work": "The related work section discusses two main areas: the application of LLMs in OR and existing research on explainable optimization. LLMs have shown promise for automating modeling and improving efficiency, with recent studies focusing on generating code for OR problems using models like GPT. Nevertheless, their primary contribution remains in automating tasks rather than providing explanations, indicating a gap in interpretability. Existing explanations for OR are limited, often addressing only simple 'what-if' analyses or sensitivity analyses, primarily focusing on parameter variations rather than complex, constraint-driven changes. Some notable works include Cyras et al.'s argumentation-based scheduling, Li et al.'s OptiGuide for supply chain scenarios, and Erwig & Kumar's combinatorial explanations. However, these approaches mainly handle easier cases, lack the ability for real-time complex scenario modifications, and fail to incorporate the advanced modeling strengths of LLMs. The authors position their work as a comprehensive framework that combines LLMs with structured quantitative analysis, specifically targeting the explainability of complex constraint modifications—integrating decision information analysis through bipartite graph representations, and providing a new evaluation benchmark.",
        "background": "The background section contextualizes the role of LLMs and explainability in OR. It acknowledges the recent advancements of LLMs like GPT-4 in automating OR tasks, including problem formulation and code generation, which improve efficiency. Despite these developments, the section notes that few efforts have focused on explainability, especially under complex or real-time scenarios. Prior work in explainability largely revolves around simple what-if or sensitivity analyses that evaluate the effects of parameter variations, but typically ignore complex constraint modifications, which are crucial for reflecting real-world operational changes such as closing facilities or adjusting resource limits. The background emphasizes the need for a more robust, quantitative, and flexible approach to explainability that can handle complex changes in models, providing deeper insights into the decision rationale. It also introduces the concept of 'Decision Information', which captures key elements of user queries that alter the decision-making context, such as specific constraints, and highlights the limitations of current sensitivity analysis methods in evaluating the importance or impact of these changes.",
        "method": "The methodology begins by formalizing the problem of explainable OR, where an existing OR problem and a user query specifying changes (like added, removed, or modified constraints) are inputs, and the goal is to generate explanations about the resulting solutions. The authors introduce the concept of 'Decision Information', representing key parameters and constraints specified in user queries, which influence the update of the OR model. They propose converting both original and updated models into a standardized linear program (LP) format and then transforming these LPs into bipartite graphs that distinguish between decision variables and constraints. The key contribution is using Graph Edit Distance (GED) to quantify the differences between the graphs, providing a measure of 'Decision Information' change. This quantification enables the explanation system to identify the impact of complex constraint modifications. The EOR framework employs an end-to-end process with three agents—Commander, Writer, and Safeguard—where user queries are received, code is generated and verified for safety, and explanations for code updates and outcomes are produced. The explanations are categorized into correctness (validating code modifications) and results (interpreting the solutions). The framework emphasizes a structured, modular approach, ensuring transparency, safety, and detailed explanation generation. The quantitative analysis of decision information using LP conversion, bipartite graph transformation, and GED provides a rigorous basis for comparing pre- and post-update models, which the LLMs leverage to generate meaningful explanations.",
        "experiments": "The experiments section details the development of a novel industrial benchmark dataset designed to evaluate explainability in OR, which addresses gaps in existing open-source datasets. The benchmark consists of 30 problems spanning diverse domains like supply chain, logistics, and finance, each paired with 10 queries involving complex modifications, such as adding, deleting, or updating constraints. The dataset includes problem descriptions, code solutions using Gurobi, and ground truth labels, all validated by OR experts. The evaluation methodology focuses on two key aspects: modeling accuracy (how well solutions adhere to optimized results) and explanation quality (clarity, relevance, and correctness). Because multiple code solutions can yield identical results, accuracy is measured by comparing outcomes rather than code structures. For explanation quality, both automated (using large language models) and expert evaluation are employed to assess how well explanations convey the rationale, correct the code, and interpret results. The experiments test different baseline methods—Standard approach (using proprietary LLMs), OptiGuide, and the proposed EOR—across multiple LLM versions (GPT-4, GPT-4-1106, GPT-4-0125, GPT-4-Turbo) in both zero-shot and one-shot settings, with hyperparameters fixed for fairness.",
        "results": "The results demonstrate that the proposed EOR framework consistently outperforms baseline methods in both modeling accuracy and explanation quality. In the zero-shot setting, EOR achieves an average accuracy of over 75%, with GPT-4-Turbo reaching 88.33%, significantly higher than Standard and OptiGuide. With one-shot learning, accuracy improves further, with EOR reaching up to 95.33%. The models including GPT-4-Turbo show robustness in adapting to the explanations task. Explanation quality assessments via expert scoring reveal that EOR provides more coherent, relevant, and detailed explanations than baselines, with higher scores in categories such as clarity and reasoning, especially evident in the automatic evaluation aligned with human judgments. Additionally, a case study illustrates how EOR offers transparent, step-by-step justification of code modifications and their impact, such as cost increases tied to added constraints, exemplifying the interpretability and practical value of the approach.",
        "discussion": "The discussion highlights the effectiveness of the EOR framework, emphasizing its capacity to produce accurate, transparent, and context-aware explanations for complex OR models. The quantitative evaluation demonstrates that EOR substantially improves over existing methods like Standard and OptiGuide across multiple metrics, underscoring the importance of integrating structured decision information analysis via bipartite graphs and GED. The superiority in explanation quality suggests that leveraging LLMs for detailed, context-aware explanations can significantly enhance user trust and understanding. While the approach is robust, limitations include computational overhead associated with graph comparison metrics and dependency on high-quality queries and problem descriptions. The authors discuss potential extensions, such as improving the efficiency of graph-based influence measurement, expanding the benchmark with more diverse scenarios, and exploring more sophisticated methods for evaluating explanation quality. The discussion also touches on future directions like applying the framework in real-time decision environments, refining automatic evaluation metrics, and integrating user feedback for iterative improvement.",
        "conclusion": "The paper concludes by summarizing the contributions of the EOR framework as a comprehensive solution for explainability in OR, grounded in the concept of 'Decision Information' and leveraging bipartite graphs and LLMs. EOR advances the state of the art by enabling rigorous, quantitative analysis of model changes, improving the transparency and interpretability of complex optimization tasks. The development of a new industrial benchmark further establishes a standardized evaluation method. Future work aims to enhance modeling accuracy, develop better automated evaluation techniques for explanations, and expand the framework's applicability in dynamic, real-world decision-making scenarios."
      }
    },
    {
      "title": "Efficient Reinforcement Learning with Large Language Model Priors",
      "abstract": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90\\% in offline learning scenarios.",
      "citationCount": -1,
      "reviews": [
        {
          "review": "- the method is simple and makes intuitive sense. Its benefits are clearly demonstrated, although in fairly simple text-based environments. Although other works have proposed ways to incorporate LLM priors into RL systems, to my knowledge this particular approach is new. \n- the paper is overall clearly written and easy to understand. I have a few suggestions below, but they are relatively minor.- the method is limited to settings where the observation consists of text, which is a big limitation. One could imagine workarounds to apply this to images, where one first uses a pretrained captioning model to extract textual captions and then feed these to the LLM. However, it’s not clear if this would give enough granularity to produce a reasonable action prior and this would need to be checked experimentally. \n- the method, as I understand it, will have difficulty scaling to high-throughput RL settings because it requires a call to the LLM for each observation encountered by the agent (which is computationally expensive). This also limits its applicability to small tasks or offline settings where the LLM outputs can be precomputed in parallel and cached. Are there any ways to address this? For example, could you distill the LLM action prior into a lightweight model and use this instead?\n- there are other ways to incorporate LLM priors into RL systems, namely through the reward function rather than the action distribution. See for example the references below [1, 2, 3, 4] (this is not a complete list). It is currently unclear what the pros/cons of the proposed method are compared to this other family of approaches. It would be helpful to include at least one method from this family as a comparison in the experiments. Also, these are not discussed in the related work section.  \n\n\n[1] Eureka: https://arxiv.org/abs/2310.12931\n[2] Language2Reward: https://arxiv.org/abs/2306.08647\n[3] Motif: https://arxiv.org/abs/2310.00166\n[4] Reward Design with LLMs: https://arxiv.org/abs/2303.00001",
          "rating": 6
        },
        {
          "review": "This paper tackles a very important topic: how to leverage LLMs into sequential decision-making tasks incorporating language.\nI enjoyed reading it.\nPerformance improvements over considered baselines are impressive.### Value-based version\n\nI am having trouble understanding how useful the value-based formulation is. What I mean is that, from my understanding, at inference time you need both the frozen LLM and the trained Q-network (which is a smaller LLM). This makes the approach quite expensive regarding memory. Regarding inference, it is also costly: the frozen LLM must provide k outputs before being able to use the Q-network LLM, for each step in the environment.\nA good comparative study here to motivate this approach could be to have a baseline which discards the frozen-LLM. What happens in this scenario ? Is it what you call the DQN baseline ? Here, it would be interesting to see what happens if you use a larger LLM during training (something which induces a memory burden comparable to using a frozen LLM + a smaller trained Q-net LLM).\n\n### Comparative study with GFlan from carta et al\n\nThe only baseline considered from the litterature is GFlan from carta et al. While I am not expert enough to assess whether this is sufficient, I have concerns about this comparative study. \nIn carta et al, GFlan is evaluated on a text-based version of the BabyAI gridworld environment:\n\nWhy not featuring this environment in the study ?\n\nAny intuitions on why GFlan is failing completely in ALFWorld (TextWorld) while it does maintain some performances in Overcooked ?\n\n\n\n\n\n\n### Clarity\n\nWhile the paper is mostly clear, I had trouble understanding the exact training and inference pipeline. The paper could benefit an update of figure 1 (along with providing the policy-based version of figure 1 somewhere).The right side of Figure 1 could be improved: in the current version, it is hard to visually understand that the Q function is used to weight each input actions and that these weights are used for proportional sampling of the next action. Also in figure 1 it is hard to understand that there are two pre-trained LLMs being used, one frozen generating an action subset, and a smaller one trained and used as a critic. I only understood that there was two pretrained LLMs being used in the DQN-Prior baseline at line 395 \"... by combining the Q-function, trained with Qwen-1.5 7B\". I would suggest making it clear early on.\n\nl.97-99 \"we first generate a free-form output from the LLM, for example, a 7B LLM, which is then mapped to an executable action through a simple rule-based projection.\" \n--> I would recommend moving at least part of what is discussed in the appendix into the main body of the paper, to be clearer about this. \nRelated to this: I only understood that you repeat this output-to-action mapping multiple times at line 166-167. It would be clearer to mention it sooner in the paper.\n\n\n### Minor / Typos\n\nThere are 3 acronym definitions in the abstract, which is a lot. Consider reducing this number, e.g. dropping SDM which is used only once.\n\nl31. \"human-AI dialogue (McTear, 2022; Li et al., 2019),\" --> make sure to sort citations in ascending order wrt year. This error appears multiple times.\n\nl. 232 --> (Snell et al.) missing year in citation.\n \nl.104 \"by (Levine, 2018)\" --> use \\cite not \\citep when directly mentioning the citation in the sentence.\n\nl.304 \"can be found in the Appendix\" -->  refer to the appropriate appendix section(s) rather than a general reference\n\nl.323 \"while avoiding full\" --> falling\n\nl.352 \"Behavior Clone(BC).\" --> Behavioral Cloning ?\n\nResults-related figures could be moved closer to the result analysis in section 4.3 to simplify reading\n\nl.404 \"k = 1 indicates the performance of the LLM on its own, without involving the Q-function\" \n--> Why calling this scenario k=1 and not k=0 ? you could theoretically use your frozen LLM to create a single action subset.",
          "rating": 6
        },
        {
          "review": "The paper is written very clearly with appropriate figures, well-formatted equations and more importantly lucid logical flow. The contributions of the paper are specified along with particular sections corresponding to individual contribution. Preliminaries provide enough details and are not overexplained. \n\nThe core idea of the paper is pretty neat -- LLMs are trained on large amount of data and they have knowledge about tasks if not fine-grained controllability; this knowledge is useful to guide an RL training. \n\nI like how the value-based implementation is designed -- sampling a set of actions using LLM prior, taking Q estimates, sampling actions using their Q estimates. The method is well grounded in the theory where direct posterior sampling is shown to be proportional to LLM prior multiplied with soft-Q distribution. The idea being simple, can be plugged with both online and offline Q-learning approaches.\n\nThe versatility of the idea is shown by applying it to policy-based RL. The LLM prior guided learning takes form similar to KL-regularized RL, hence modifying PPO in a straightforward way would lead to easy access to prior knowledge. However, I do I have some contentions around the novelty of this part which is discussed in weakness section.\n\n\nThe choice of the experimental setup is also very apt. The environments are chosen so that LLMs can provide textual action descriptions which then can be executed through text to action conversion in the environment's processing.Practicality of value based approaches: Although the value-based direct posterior sampling technique described in the paper is logically sound, I wonder about its practicality. Mainly, value-based approaches when applied to combinatorically large action spaces like text generation is practically infeasible. Hence, the paper's proposed method might be hard to apply beyond toy textual environments. Noticeably, these environments are designed to accept actions in a particular format or use rigorous action decoding, etc. to deal with the combinatoric complexity. Such environments are rare in reality.\n\nProblems with choice of prior inducing LLMs: Two LLMs might produce semantically same action but different text. I am curious to know whether the Q-learning based approaches would assign similar values to both. In general, I feel that prior inducing LLM and its compatibility with the environment's action decoding plays major role when they are used to train RL policies in text-based sequential decision making. \n\nNovelty of the additional KL penalty in policy-based RL: ELBO-based RL approaches [1, 2] have been already established in the RL literature. The additional KL-penalty proposed in the current paper is based on the same. In single step decision making, the KL-penalty on an LLM prior would be exactly same as KL-penalty used along with supervised fine-tuned (SFT) policy. Thus, in that setting, the paper's technique lacks novelty. Now, coming to the multi-step decision making, unless the proposed direct posterior sampling is also applied on top of PPO, additional KL-penalty is equivalent to using a good enough prior policy (e.g. behavior cloned policy). Therefore, the novelty of the paper is limited in policy-based RL.\n\nCombined together,  I find that paper provides great insights into a direct posterior sampling paradigm for using prior knowledge present in the LLMs, but the approach might have limited applicability beyond hand-designed textual environments, its applicability is limited by limitations of value-based approaches, and its novelty in policy-based RL is not significant.",
          "rating": 8
        },
        {
          "review": "1. This paper is clearly written and easy to follow.\n2. The considered \"incorporating the background knowledge of LLM into RL\" is an interesting topic.1. There are no convergence guarantees of sampling using LLM as prior for Exploration and Q-function update. Moreover, there is no guarantee that learning the Q-function as shown in Equation (7) will give us a conservative Q-function.\n2. If the LLM can not provide a good prior, constraining the learned policy as shown in Equation (8) will result in sub-optimal policies.\n3. The RL process has to query the LLM each time we update the policy, which is time-consuming and computation-consuming.",
          "rating": 5
        }
      ],
      "meta_review": "The authors propose to use LLM priors to guide policies in more efficient exploration. Specifically, they use Bayesian inference to integrate action distribution priors provided by an LLM and perform posterior sampling. They show how this can be done in both policy-based and value-based RL frameworks, and demonstrate improved sample efficiency in simple, discrete-action environments. \n\nReviewers found the paper easy to follow and clearly written. The core idea is simple and neat, and performance improvements over the chosen baselines are impressive. However, there were some reviewer concerns about the simplicity of the environments chosen for the experiments, and that the framework does not allow for continuous action environments. There was also no discussion in related work of how this framework compares to providing LLM priors via a reward function. \n\nThe authors provided additional experiments during the rebuttal phase that assuaged some reviewers' concerns. Given the nice idea, provision of both value-based and policy-based frameworks for incorporating prior action distributions, and alternative perspective from the more common reward-based framework, I vote to accept this paper.\nThere was some disagreement in the reviewer discussion about the paper. Reviewer h6X1 points out that lack of expertise by the LLM will lead to suboptimal policies, and that the proposed methods are limited to discrete action space. However, reviewer uGBu believes the results are convincing enough in demonstrating the approach's merits and give hints of wider applicability to meet the bar for acceptance, and reviewer vRpg agrees.",
      "replies": [
        {
          "summary": "This paper proposes an approach to leverage LLM priors for RL, both in the online and offline settings.\nThe setting is restricted to text-based observations. Specifically, they estimate a prior p(a|s) by sampling K actions from the LLM conditioned on the state, and use these to either restrict the Q-values to the actions with non-zero probability (for online or offline Q-learning based methods), or to enforce a KL penalty with respect to the policy’s distribution over actions. They evaluate on several text-based environments and show improvements in terms of sample efficiency over vanilla RL. Overall, the paper does a decent job of illustrating the benefits of the method within its scope, but the scope itself is quite limited (in terms of broad applicability and scalability - more on this below). Also, there are a number of alternate methods for incorporating LLM priors into RL systems, that are not discussed or compared to.\n\n=====\nUpdate after rebuttal: the authors have added several additional experiments which partially address the initial limitations I discussed. Although I still think the environments are on the simple side, and the method in its current form is difficult to scale to high-throughput settings, the results are convincing enough in demonstrating the approach's merits and give hints of wider applicability to meet the bar for acceptance.",
          "soundness": 3,
          "presentation": 3,
          "contribution": 3,
          "strengths": "- the method is simple and makes intuitive sense. Its benefits are clearly demonstrated, although in fairly simple text-based environments. Although other works have proposed ways to incorporate LLM priors into RL systems, to my knowledge this particular approach is new. \n- the paper is overall clearly written and easy to understand. I have a few suggestions below, but they are relatively minor.",
          "weaknesses": "- the method is limited to settings where the observation consists of text, which is a big limitation. One could imagine workarounds to apply this to images, where one first uses a pretrained captioning model to extract textual captions and then feed these to the LLM. However, it’s not clear if this would give enough granularity to produce a reasonable action prior and this would need to be checked experimentally. \n- the method, as I understand it, will have difficulty scaling to high-throughput RL settings because it requires a call to the LLM for each observation encountered by the agent (which is computationally expensive). This also limits its applicability to small tasks or offline settings where the LLM outputs can be precomputed in parallel and cached. Are there any ways to address this? For example, could you distill the LLM action prior into a lightweight model and use this instead?\n- there are other ways to incorporate LLM priors into RL systems, namely through the reward function rather than the action distribution. See for example the references below [1, 2, 3, 4] (this is not a complete list). It is currently unclear what the pros/cons of the proposed method are compared to this other family of approaches. It would be helpful to include at least one method from this family as a comparison in the experiments. Also, these are not discussed in the related work section.  \n\n\n[1] Eureka: https://arxiv.org/abs/2310.12931\n[2] Language2Reward: https://arxiv.org/abs/2306.08647\n[3] Motif: https://arxiv.org/abs/2310.00166\n[4] Reward Design with LLMs: https://arxiv.org/abs/2303.00001",
          "questions": "If the authors can address any number of the points raised in the Weaknesses section, I would be willing to raise my score accordingly. I also have a few minor comments below:\n\n- In Section 4.1, it would be helpful to give some more detail on the environments you use. The current descriptions are quite high level and not everyone may be familiar with these environments or have intuitions about them. Maybe include a figure with example observations/actions for each one - you still have half a page under the page limit so there should be space. \n- Some of the references are missing dates, for example (Snell et al), line 232. \n- This is a bit of a nitpick, but in Figure 1, is there a reason the OpenAI logo is on the LLM box? My understanding is that this paper does not use OpenAI’s LLMs, and instead uses open models (Qwen and Llama). I don’t think an institutional logo is necessary, but if you do want to include one, it would seem more fair to give credit to those that made the models that you use available.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 6,
          "confidence": 4,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "Authors present a new way to train LLM-based agents in interactive sequential environments. Their approach is based on leveraging a potentially large frozen LLM to generate action subsets for encountered states. The action subset is then used to simplify the RL finetuning of a smaller LLM. Their approach can be adapted to value-based (giving a subset of actions for the LLM Q-network) and policy-based scenarios (constraining the LLM policy with a KL-div loss over the subset of actions). Authors showcase how their approach outperforms baselines in TextWorld and Overcooked, both in online and offline learning settings.",
          "soundness": 3,
          "presentation": 2,
          "contribution": 1,
          "strengths": "This paper tackles a very important topic: how to leverage LLMs into sequential decision-making tasks incorporating language.\nI enjoyed reading it.\nPerformance improvements over considered baselines are impressive.",
          "weaknesses": "### Value-based version\n\nI am having trouble understanding how useful the value-based formulation is. What I mean is that, from my understanding, at inference time you need both the frozen LLM and the trained Q-network (which is a smaller LLM). This makes the approach quite expensive regarding memory. Regarding inference, it is also costly: the frozen LLM must provide k outputs before being able to use the Q-network LLM, for each step in the environment.\nA good comparative study here to motivate this approach could be to have a baseline which discards the frozen-LLM. What happens in this scenario ? Is it what you call the DQN baseline ? Here, it would be interesting to see what happens if you use a larger LLM during training (something which induces a memory burden comparable to using a frozen LLM + a smaller trained Q-net LLM).\n\n### Comparative study with GFlan from carta et al\n\nThe only baseline considered from the litterature is GFlan from carta et al. While I am not expert enough to assess whether this is sufficient, I have concerns about this comparative study. \nIn carta et al, GFlan is evaluated on a text-based version of the BabyAI gridworld environment:\n\nWhy not featuring this environment in the study ?\n\nAny intuitions on why GFlan is failing completely in ALFWorld (TextWorld) while it does maintain some performances in Overcooked ?\n\n\n\n\n\n\n### Clarity\n\nWhile the paper is mostly clear, I had trouble understanding the exact training and inference pipeline. The paper could benefit an update of figure 1 (along with providing the policy-based version of figure 1 somewhere).The right side of Figure 1 could be improved: in the current version, it is hard to visually understand that the Q function is used to weight each input actions and that these weights are used for proportional sampling of the next action. Also in figure 1 it is hard to understand that there are two pre-trained LLMs being used, one frozen generating an action subset, and a smaller one trained and used as a critic. I only understood that there was two pretrained LLMs being used in the DQN-Prior baseline at line 395 \"... by combining the Q-function, trained with Qwen-1.5 7B\". I would suggest making it clear early on.\n\nl.97-99 \"we first generate a free-form output from the LLM, for example, a 7B LLM, which is then mapped to an executable action through a simple rule-based projection.\" \n--> I would recommend moving at least part of what is discussed in the appendix into the main body of the paper, to be clearer about this. \nRelated to this: I only understood that you repeat this output-to-action mapping multiple times at line 166-167. It would be clearer to mention it sooner in the paper.\n\n\n### Minor / Typos\n\nThere are 3 acronym definitions in the abstract, which is a lot. Consider reducing this number, e.g. dropping SDM which is used only once.\n\nl31. \"human-AI dialogue (McTear, 2022; Li et al., 2019),\" --> make sure to sort citations in ascending order wrt year. This error appears multiple times.\n\nl. 232 --> (Snell et al.) missing year in citation.\n \nl.104 \"by (Levine, 2018)\" --> use \\cite not \\citep when directly mentioning the citation in the sentence.\n\nl.304 \"can be found in the Appendix\" -->  refer to the appropriate appendix section(s) rather than a general reference\n\nl.323 \"while avoiding full\" --> falling\n\nl.352 \"Behavior Clone(BC).\" --> Behavioral Cloning ?\n\nResults-related figures could be moved closer to the result analysis in section 4.3 to simplify reading\n\nl.404 \"k = 1 indicates the performance of the LLM on its own, without involving the Q-function\" \n--> Why calling this scenario k=1 and not k=0 ? you could theoretically use your frozen LLM to create a single action subset.",
          "questions": "l.286 \"We plot the rewards averaged over the final third of the training processes for trainable baselines\" --> Evaluation of a model performance is not performed by conducting periodic performance measurements over a test set of tasks ? Could you elaborate on how performance is measured ?\n\nIn your policy-based version, which you present in section 3.3 as an extension of the work of Carta et al., 2023, I am not sure to understand what is trained is what is fixed. Are you, like in Carta et al, training all parameters of the LLM ? What is being trained by your modified PPO loss ? Would be good to have the policy-based version of figure 1 somewhere. My understanding after reading section 4.2 is that you have a large fixed pretrained model used to compute action priors, and you train another smaller Flan-T5 as the policy.\n\nIn your offline learning experiments, by looking at table 1 it looks like you are applying a vanilla DQN to offline datasets. It comes with the risk of overestimating values of actions that are not supported by the dataset. Would it make more sense to use value-based methods designed for offline learning, e.g. Implicit Q Learning from Kostrikov et al ?\n\nIn your offline learning results, I see that CQL underperforms wrt Behavioral Cloning. This is unusual: any ideas on why ?\n\n\n\n\nGiven the aforementioned questions and potential limitations of this paper, I will go for a weak reject, and look forward to the discussion to update my score.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 6,
          "confidence": 4,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "In this work, the authors propose idea of using large language models (LLMs) for providing prior information about execution of a task, which simplifies exploration in reinforcement learning (RL) and improves sample efficiency. The core idea is to treat LLMs as prior distribution over actions and then apply variational inference in conjunction with posterior sampling. The authors demonstrate this technique on both value-based and policy-based approaches. In value-based implementation, a subset of the action space is chosen for Q-learning instead of using entire action space. This subset selection is done using LLM prior. Similarly, in the policy-based implementation, a KL-penalty is introduced with respect to the prior LLM action distribution, but implemented differently from regular KL penalty in RL from human feedback (RLHF), by posterior sampling of multiple candidate actions. The experiments are conducted on text-based RL environments with large action spaces and multi-step horizon (note this is crucial for demonstrating the difference against RLHF techniques). Value-based baselines include DQN for online learning and CQL for offline learning. For policy-based baseline, PPO algorithm is compared against.\n\nThe results demonstrate the superiority of the approach in (1) overall returns, (2) generalizing beyond seen tasks, (3) sample efficiency.",
          "soundness": 4,
          "presentation": 4,
          "contribution": 3,
          "strengths": "The paper is written very clearly with appropriate figures, well-formatted equations and more importantly lucid logical flow. The contributions of the paper are specified along with particular sections corresponding to individual contribution. Preliminaries provide enough details and are not overexplained. \n\nThe core idea of the paper is pretty neat -- LLMs are trained on large amount of data and they have knowledge about tasks if not fine-grained controllability; this knowledge is useful to guide an RL training. \n\nI like how the value-based implementation is designed -- sampling a set of actions using LLM prior, taking Q estimates, sampling actions using their Q estimates. The method is well grounded in the theory where direct posterior sampling is shown to be proportional to LLM prior multiplied with soft-Q distribution. The idea being simple, can be plugged with both online and offline Q-learning approaches.\n\nThe versatility of the idea is shown by applying it to policy-based RL. The LLM prior guided learning takes form similar to KL-regularized RL, hence modifying PPO in a straightforward way would lead to easy access to prior knowledge. However, I do I have some contentions around the novelty of this part which is discussed in weakness section.\n\n\nThe choice of the experimental setup is also very apt. The environments are chosen so that LLMs can provide textual action descriptions which then can be executed through text to action conversion in the environment's processing.",
          "weaknesses": "Practicality of value based approaches: Although the value-based direct posterior sampling technique described in the paper is logically sound, I wonder about its practicality. Mainly, value-based approaches when applied to combinatorically large action spaces like text generation is practically infeasible. Hence, the paper's proposed method might be hard to apply beyond toy textual environments. Noticeably, these environments are designed to accept actions in a particular format or use rigorous action decoding, etc. to deal with the combinatoric complexity. Such environments are rare in reality.\n\nProblems with choice of prior inducing LLMs: Two LLMs might produce semantically same action but different text. I am curious to know whether the Q-learning based approaches would assign similar values to both. In general, I feel that prior inducing LLM and its compatibility with the environment's action decoding plays major role when they are used to train RL policies in text-based sequential decision making. \n\nNovelty of the additional KL penalty in policy-based RL: ELBO-based RL approaches [1, 2] have been already established in the RL literature. The additional KL-penalty proposed in the current paper is based on the same. In single step decision making, the KL-penalty on an LLM prior would be exactly same as KL-penalty used along with supervised fine-tuned (SFT) policy. Thus, in that setting, the paper's technique lacks novelty. Now, coming to the multi-step decision making, unless the proposed direct posterior sampling is also applied on top of PPO, additional KL-penalty is equivalent to using a good enough prior policy (e.g. behavior cloned policy). Therefore, the novelty of the paper is limited in policy-based RL.\n\nCombined together,  I find that paper provides great insights into a direct posterior sampling paradigm for using prior knowledge present in the LLMs, but the approach might have limited applicability beyond hand-designed textual environments, its applicability is limited by limitations of value-based approaches, and its novelty in policy-based RL is not significant.",
          "questions": "Questions and Suggestions:\n\n- Should the proportionality sign on line 147 be equal to? I understand the sentence is correct in its current form.\n\n- The $p$ is overloaded on lines 112-113, especially, please change the \"$p_{LLM}$ as the action prior $p$\" sentence where the latter $p$ is said to be not confused with $p$ used for probability of start state or action given a state in the definition of probability of trajectory.\n\n- I am not sure by what is meant by \"Denote that the above …\" in Proposition 1. Please rewrite this for clarification.\n\n- Line 215 - \"reply buffer\" should be changed to \"replay buffer\" \n\n- Is the gamma in equation 8 the same as the discount factor?\n\n- Line 323 - \"avoiding full into holes\". Please rectify this.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 8,
          "confidence": 4,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "This paper considers treating LLMs as prior action distributions and integrating them into RL frameworks. Specifically, authors proposes to use LLM for exploration,  Q target calculation and imposing conservatism. Experiments on the ALFWorld and Overcooked environments illustrates the effectiveness of the proposed framework.",
          "soundness": 2,
          "presentation": 3,
          "contribution": 2,
          "strengths": "1. This paper is clearly written and easy to follow.\n2. The considered \"incorporating the background knowledge of LLM into RL\" is an interesting topic.",
          "weaknesses": "1. There are no convergence guarantees of sampling using LLM as prior for Exploration and Q-function update. Moreover, there is no guarantee that learning the Q-function as shown in Equation (7) will give us a conservative Q-function.\n2. If the LLM can not provide a good prior, constraining the learned policy as shown in Equation (8) will result in sub-optimal policies.\n3. The RL process has to query the LLM each time we update the policy, which is time-consuming and computation-consuming.",
          "questions": "1. The experiments are only conducted on simple environments with discrete action spaces. What if continuous action space (e.g., offline RL on the D4RL benchmark)?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 5,
          "confidence": 4,
          "code_of_conduct": "Yes"
        }
      ],
      "contribution": "In summary, our main contributions are three-fold: 1. We present a unified framework for integrating Large Language Models (LLMs) as probabilistic priors into Markov decision-making frameworks. **(Section 2)** 2. We practically implement the framework by leveraging LLMs as the refined action sampler for value-based online RL **(Section 3.1)**, offline RL **(Section 3.2)** or behavior regularizer for policy-based RL **(Section 3.3)**. 3. Extensive experiments on ALFWorld and Overcooked demonstrate that our new framework can significantly boost sample efficiency compared with both pure RL and pure LLM baselines, and also bring in more robust and generalizable value function. **(Section 4)**",
      "paper_id": "e2NRNQ0sZe",
      "pdf": "/pdf/a04e7d533106508961e26000bf913788bc029779.pdf",
      "venue": "ICLR 2025 Poster",
      "summary": {
        "introduction": "The introduction emphasizes the significance of sequential decision-making tasks in real-world applications like robotics, autonomous driving, and human-AI interaction. Traditional methods such as reinforcement learning (RL), heuristic search, and optimal control have achieved notable success but face issues with large computational complexity and poor generalizability. The emergence of large language models (LLMs) offers a new direction, as they possess vast domain knowledge and reasoning abilities. The paper explores how to leverage LLMs as priors in RL frameworks to improve efficiency and generalization. The authors propose treating LLMs as prior action distributions and integrating them into RL models via Bayesian inference, utilizing variational inference and posterior sampling. This approach is aimed at reducing exploration complexity and enhancing sample efficiency. The introduction concludes with the main contributions, including a unified framework for LLM priors, practical implementations in both policy-based and value-based RL, and empirical validation on benchmarks showing significant improvements in sample efficiency.",
        "related_work": "The related work section discusses the landscape of LLM-driven agents and their use in decision-making tasks. It mentions methods where LLMs are used directly via prompting, such as human-crafted prompts, and search-based methods like Tree-of-Thoughts and Monte Carlo Tree Search to enhance reasoning. It also highlights research integrating LLMs with reinforcement learning, including fine-tuning approaches (RLFT), and the use of LLMs as world models or behavior regularizers within RL contexts. The authors note that their work differs by viewing the LLM primarily as an action prior and performing Q-learning within this prior space. Further, the section covers the probabilistic interpretation of LLMs, emphasizing their Bayesian properties. It references works that interpret in-context learning and chain-of-thought reasoning as Bayesian inference, and approaches that model LLMs as probabilistic generators for decision-making. The section concludes by positioning their work as an effort to blend Bayesian inference with LLM priors to improve decision-making efficiency.",
        "background": "This section formulates the problem of utilizing LLMs in sequential decision-making as a Bayesian inference task within the framework of Markov Decision Processes (MDPs). The MDP includes textual state and action spaces, transition kernels, and rewards. The key idea is to treat the LLM as an action prior, providing a distribution over possible actions given a state. Since LLMs can't directly produce executable actions reliably, their outputs are mapped to discrete actions via rule-based projection after prompting with state and admissible actions. The authors introduce a variational inference perspective, where the goal is to maximize the probability of achieving optimal trajectories, factoring in the LLM as a prior. They define trajectory priors and variational distributions, and derive a step-wise objective involving reward maximization regularized by a KL divergence term against the LLM prior. They also describe an alternative method based on direct posterior inference, where actions are sampled from a reweighted distribution based on Q-values and the likelihood of optimality, which is proportional to the exponential of discounted rewards. This posterior inference strategy involves generating candidate actions from the LLM prior, estimating their Q-values, and selecting actions via a softmax-weighted sampling, approximating the optimal posterior over actions at each step.",
        "method": "The methods section details practical strategies for integrating LLM priors into RL frameworks, covering value-based, offline, and policy-based approaches. For online value-based RL, the authors adapt Deep Q-Network (DQN) to a variant called DQN-Prior, where exploration and updates occur within a reduced action space sampled from the LLM prior. Actions are sampled from the prior, evaluated with Q-values, and selected via a softmax distribution. The Q-network encodes state-action pairs with a frozen BERT, with a lightweight MLP on top, avoiding fine-tuning the language model itself. Offline RL methods are modified similarly: they introduce CQL-Prior, which restricts the Bellman update and overestimation penalty to the prior subspace, thus reducing Q-value overestimation and sample complexity. For policy-based RL, they adapt the GFlan approach, where a small language model policy is fine-tuned via PPO with a KL regularizer towards the LLM prior policy, derived from the variational inference framework and posterior sampling ideas. The training involves sampling action proposals from the prior, estimating Q-values, and performing policy updates with an added KL constraint to incorporate prior knowledge. The section emphasizes that all methods leverage the LLM as a guiding prior to both reduce the search space and regularize the policy's behavior.",
        "experiments": "The experiments aim to validate the effectiveness of incorporating LLM priors into RL algorithms in both online and offline settings across multiple environments. The environments include ALFWorld, Overcooked, and Frozen Lake—covering text-based games and gridworlds with finite action spaces. The baselines consist of pure RL algorithms (DQN, CQL), offline RL (CQL, behavioral cloning), and policy RL (GFlan), with variations that incorporate LLM priors (DQN-Prior, CQL-Prior, GFlan-Prior). The experimental setup involves using Qwen-1.5 models as the core LLM prior, with action proposal sizes (k=5) or other values, and different LLM sizes for different tasks. Evaluation metrics include cumulative rewards and success rates. Extensive ablation studies examine the impact of the number of action proposals (k), KL regularization coefficients, softmax temperature parameters, and generalization to unseen tasks and diverse LLMs. The experiments demonstrate that LLM priors significantly improve sample efficiency, reduce training data requirements, and improve generalization in both online and offline RL, confirming the theoretical benefits discussed. The offline results show that restricting Q-value updates to the prior action space leads to faster learning and less overestimation, saving over 90% of sample data in some tasks. The offline evaluation also indicates that training with a larger, more powerful LLM as the prior enables better decision-making with smaller models during deployment.",
        "results": "The results section presents empirical evidence that integrating LLM priors into RL algorithms substantially enhances their efficiency and performance. In value-based RL, the DQN-Prior variant, which operates within a restricted action space sampled from the LLM prior, outperforms traditional DQN and other baselines, particularly in environments with large action spaces like ALFWorld, by reducing the exploration space and enabling faster policy learning. Offline RL experiments with CQL-Prior show that leveraging the prior reduces the number of samples needed to reach high rewards by over 90%, making offline learning more feasible and data-efficient. In policy-based settings, GFlan-Prior successfully incorporates the LLM prior into PPO, leading to performance comparable or superior to baselines, with hyperparameter sensitivity analyses demonstrating the importance of the prior regularization coefficient and the number of action proposals. The generalization experiments indicate that policies trained with LLM priors can perform well on unseen tasks and with different LLM-sized priors, highlighting the robustness and transferability of the approach. Overall, the results validate that the use of fixed LLM priors as action proposals or regularizers shrinks the exploration space, aligns the policy with prior knowledge, and yields significant gains in sample and data efficiency.",
        "discussion": "The discussion interprets the experimental findings, emphasizing that using LLMs as priors effectively narrows the exploration space and guides RL algorithms towards more promising actions. This reduces sample complexity and enhances generalization across tasks. The authors note that the success hinges on the quality of the LLM prior and the setting of hyperparameters; a well-aligned prior can significantly boost learning efficiency. They highlight the flexibility of the framework, which can be applied to both online and offline RL, and to different types of algorithms, including value-based and policy-based methods. The discussion acknowledges limitations, such as reliance on finite action spaces and the need for mapping LLM outputs to discrete actions. Future work will explore continuous action spaces, prompt engineering for better priors, and extending the framework to more complex, real-world problems. The authors underscore that their approach demonstrates the potential of integrating probabilistic prior knowledge from large models into RL to achieve more scalable and robust decision-making systems.",
        "conclusion": "The conclusion summarizes that the paper presents a novel approach for enhancing reinforcement learning by using large language models as action priors from a Bayesian inference standpoint. This framework simplifies the decision-making process by reducing exploration bounds or regularizing policy behavior, leading to markedly improved sample efficiency. Empirical results across multiple benchmarks confirm that integrating fixed LLM priors into both value-based and policy-based RL accelerates learning and enhances generalization, especially in text-based, finite action space environments. The authors highlight that their approach minimizes the need for prompt engineering and fine-tuning, making it a practical and scalable methodology. Finally, they outline future directions, including extending the approach to infinite or continuous action spaces, employing prompt-based strategies for further knowledge infusion, and deploying this framework to more complex, real-world decision tasks."
      }
    },
    {
      "title": "Difference-of-submodular Bregman Divergence",
      "abstract": "The Bregman divergence, which is generated from a convex function, is commonly used as a pseudo-distance for comparing vectors or functions in continuous spaces. In contrast, defining an analog of the Bregman divergence for discrete spaces is nontrivial. Iyer & Bilmes (2012b) considered Bregman divergences on discrete domains using submodular functions as generating functions, the discrete analogs of convex functions. In this paper, we further generalize this framework to cases where the generating function is neither submodular nor supermodular, thus increasing the flexibility and representational capacity of the resulting divergence, which we term the difference-of-submodular Bregman divergence. Additionally, we introduce a learnable form of this divergence using permutation-invariant neural networks (NNs) and demonstrate through experiments that it effectively captures key structural properties in discrete data. As a result, the proposed method significantly improves the performance of existing methods on tasks such as clustering and set retrieval problems. This work addresses the challenge of defining meaningful divergences in discrete settings and provides a new tool for tasks requiring structure-preserving distance measures.",
      "citationCount": -1,
      "reviews": [
        {
          "review": "All in all I believe that this paper provides valuable contribution to the community, as it lays the theoretical foundations for using discrete Bregman divergences in practical learning tasks, which is an intriguing approach that is not often discussed in this context.While the experiments are rudimentary, the paper offers sound theoretical results and demonstrates their applicability to practical learning tasks.\n\nMy main concern is that the paper lacks in terms of clarity, particularly in the experiment section. See details below. Should my concerns be addressed, I would be willing to raise my score.",
          "rating": 8
        },
        {
          "review": "1. The authors extend previous work on constructing Bregman divergence by relaxing the requirement that the generator function $f$ must be submodular.  \n2. They present a numerical form of the proposed discrete Bregman divergence using permutation-invariant neural networks.  \n3. In the experiment section, the authors demonstrate that the constructed Bregman divergence returns smaller values for similar set pairs and larger values for dissimilar set pairs.1. In Section 5.2, *Real Data Application*, it appears that there are no baselines for either the clustering or shape retrieval experiments. Figure 2 demonstrates the performance of the proposed Bregman divergence, but it is difficult to see how this new divergence improves upon previous divergences, such as those presented in [Iyer and Bilmes (2012a)].\n\nAdditionally, quantitative metrics (e.g., accuracy in the shape retrieval/classification experiment) and wall-clock comparisons to assess the performance of the proposed divergence should be included and discussed.\n\n2. The differences between the new divergence construction technique and previous work [Iyer and Bilmes (2012a)] remain unclear. For instance, in Equation (7), based on Iyer’s work, $f$ should be submodular, and $h_Y$ serves as its subgradient, which is straightforward to construct. \n\nIn the new divergence construction technique proposed here (presumably based on Theorem 3.1'), $f$ can be any set function, which raises the challenge of finding the appropriate modular mapping $h_Y$. In short, the old technique imposes a requirement on $f$, making $h_Y$ straightforward once $f$ is constructed. The new technique has no requirement for $f$ but requires finding an appropriate $h_Y$. Given this, it is not immediately clear why the new technique would outperform the previous one.",
          "rating": 6
        },
        {
          "review": "The paper is well-written and presented in a clear, accessible manner. The authors thoroughly acknowledge the relevant literature and prior work, thereby enhancing the clarity of their contributions. To the best of my knowledge, the proposed generalization of DBDs is novel, as is the framework introduced for learning them.The main weaknesses of the paper are twofold: (1) a lack of motivation, and, relatedly, (2) a lack of empirical evidence to demonstrate the benefits of the proposed approach. Regarding the first point, while the paper introduces a new mathematical tool—the (generalized) DBD—the motivation for its introduction is not sufficiently developed. Although the authors demonstrate that their extension allows the definition of larger classes of DBDs, the motivations for considering DBDs in the first place for ML tasks are not clearly articulated. This should be clarified in the introduction and related work. Concerning the second point, the authors aim to demonstrate the applications of this tool for clustering and retrieval tasks; however, the experimental results might be insufficient to demonstrate how the proposed generalization improves upon standard DBDs using only submodular generating functions, as only one experiment is provided to demonstrate this point. Furthermore, it remains unclear how DBDs compare to other baseline methods capable of addressing similar tasks as no comparative analysis has been carried out. To enhance the impact of their work, I suggest that the authors strengthen the motivation, and compare the proposed approach with SoTA baselines on the tasks considered.",
          "rating": 5
        },
        {
          "review": "The paper addresses a problem of extending Bregman divergences to discrete spaces. This is an important problem as many methods that rely on Bregman divergences in continuous settings may be adapted to discrete scenarios.\n\nThe submission does a good job providing a structured and clear background on the treated problem.\n\nThe central idea of the paper stems from DS decomposition. Typically, one needs a submodular function $f$ to instantiate submoduler Bregman divergence. However, the authors notice that DS decomposition proposed in prior work alleviates this constraint on $f$ itself, and propose to use the difference of two submodular functions, $f = f^1 - f^2$. I'm not an expert in the area, but it seems the idea of using DS decomposition in forming more expressive class of Bregman divergences on discrete spaces has not been explored in the literature and is novel.\n\nThe authors provide empirical validation for the proposed learning framework. Given two submodular $f^1, f^2 $ functions, DS decomposition facilitates discrete Bregman divergence specification and with proper implementation of $f^1, f^2$ one can utilize metric learning approach to learn divergence from data. This defines a natural combination of permutation-invariant neural networks (to work with sets) and triplet loss (to learn the divergence). With MNIST experiment, they illustrate that the learned divergence provides reasonable results for dissimilarity between sets of images, with similar image sets getting smaller dissimilarity scores. Point cloud dataset experiments validate that the use of novel divergence class provides benefits over the use of submodular Bregman divergences in set clustering experiment. They further validate that the divergence learned provides semantically close set retrieval.At the very start, the paper highlights the problem of identifiability of divergence, which comes from divergence definition $D(x, y) = 0 \\rightarrow x=y$. Thus, later the submission puts a lot of attention on the strict submodularity of the underlying set functions in Bregman divergences as one needs strict inequalities to satisfy the definition requirement. However, the proposed implementation seems problematic to me in this sense. Indeed, the submodularity is guaranteed, but DBD requires strict DS decomposition, so $f^i$s should be strictly submodular, which doesn't seem the case for the adopted architecture:\n$$\nf_{PN}([x_i]_{i \\in X}) = \\max_{i \\in X} ReLU(h(x_i)),\n$$\nwhere $h: \\mathcal{X} \\rightarrow \\mathbb{R}$ instantiated with an MLP with last activation set to ReLU. When argmax is achieved outside the intersection, the inequality from Definition 2.3 holds, but if argmax is inside the intersection, we fail to meet strict modularity. So the proposed implementation doesn't match the requirements of DBD. Perhaps I'm missing something here?\n\nSince the proposed approach targets practical side of things, it seems it needs more extensive experimentation. For example, one would expect to see expressiveness comparison that is not limited to only one task (set clustering) and one dataset (ModelNet40). This will help gain more empirical evidence for the substantial gain in expressiveness across tasks and task complexities, and justify the use of DBD which requires more computational resources (two functions instead of one).\n\nThe authors didn't discuss the limitations of their approach, which partly stems from limited experimentation.",
          "rating": 8
        },
        {
          "review": "see above.see above.",
          "rating": 8
        }
      ],
      "meta_review": "This paper extends Bregman divergence, typically derived from convex functions, to cases where the generating function is neither submodular nor supermodular, resulting in the difference-of-submodular Bregman divergence. This further extends the framework introduced by Iyer & Bilmes (2012b) which used submodular functions as generating functions.  A learnable version of this divergence is proposed using permutation-invariant neural networks, showing through experiments that it captures key structural properties in discrete data and improves performance in clustering and set retrieval tasks.\n\nReviewers generally agree that the paper provides a new and useful tool for tasks requiring structure-preserving distance measures, and achieving it requires overcoming a few nontrivial obstacles with new insights and techniques.  That said, the empirical evaluation can still be much improved.  Overall, the paper forms an interesting addition to the conference.\nThe rebuttal has been noted by the reviewers and have been taken into account by the AC in the recommendation of acceptance/rejection.",
      "replies": [
        {
          "summary": "The authors propose a novel method to construct Bregman divergences for functions defined on discrete sets. Their method is backed by new theory, and it is learnable and applicable to deep-learning on set data.\n\nTheir main contributions are:\n1. They provide theoretical justification for the technique of [1] to construct Bregman divergences from strictly submodular generator functions. Namely, they prove that the induced divergence is indeed a divergence.\n2. They provide a technique to construct Bregman divergences from *arbitrary* generator set-functions, which need not be submodular. They do so using a submodular analogue of the Difference-of-Convex decomposition.\n3. They motivate their construction theoretically by proving that a larger class of generator functions makes for a larger class of induced Bergman divergences.\n4. They demonstrate the applicability of their method to deep learning via preliminary experiments. In their experiments they use a neural network based on their construction, which computes discrete Bergman divergences that are learnable.\n   - They train their architecture to comptue divergences between sets of MNIST digits, and show through examples that the learned metric makes sense.\n   - They demonstrate the advantage of their construction compared to simpler ones (based solely on modular set-functions, or on basic set operations) by performing k-Means clustering on PointNet-40, treating the point clouds as sets and using the computed divergences as a distance measure.\n   - They further demonstrate their method in the task of set-retrieval on PointNet-40, showing in several examples that the top 5 retrieved examples indeed belong to the correct class.\n\n[1] Faust, Fauzi, Saunderson (2023) - A Bregman Divergence View on the Difference-of-Convex Algorithm",
          "soundness": 3,
          "presentation": 2,
          "contribution": 3,
          "strengths": "All in all I believe that this paper provides valuable contribution to the community, as it lays the theoretical foundations for using discrete Bregman divergences in practical learning tasks, which is an intriguing approach that is not often discussed in this context.",
          "weaknesses": "While the experiments are rudimentary, the paper offers sound theoretical results and demonstrates their applicability to practical learning tasks.\n\nMy main concern is that the paper lacks in terms of clarity, particularly in the experiment section. See details below. Should my concerns be addressed, I would be willing to raise my score.",
          "questions": "## Major comments\n\n1. Crucial details regarding your experiments are missing. For example:\n   - Line 359: With that loss function, it seems that the global optimum of zero is trivially attainable. What prevents your method from collapsing to zero?\n   - What architectures did you use in practice? (dimensions, numbers of layers etc.)\n   - Line 307: How did you compute the subgradients and supergradients of the neural networks a priori?\n   \n   The paper could seriously benefit from an additional appendix describing the technical details of your experiments.\n2. Line 458: \"In addition, since our DBD quantifies the differences between discrete data, it is naturally invariant to these rotations. Note that the rotational invariance in point cloud data corresponds to the permutation invariance in set data and this property is not provided by usual divergences over vectors.\"\n   This argument and reasoning are unclear. Are you saying that the DBD between two point-clouds X,Y is rotation invariant, namely D(X,Y) = D(RX,RY) for all rotations R? If this is the case, it is highly nontrivial and requires proof. Anyway the argument itself requires clarification.\n3. Line 20: \"outperforming existing methods on tasks such as clustering\" is too strong a statement, as you did not compare with existing methods outside the realm of discrete set functions.\n4. The continuous analog of the core idea in this work, namely to construct Bregman divergences from arbitrary nonconvex generator functions using the Difference-of-Convex decomposition, was studied in [1]. Their contribution should be acknowledged.\n\n## Minor comments\n\n1. How come bar-DBD with decomposition performed worse than grow- and shrink-DBD without decomposition? Is there an intuitive explanation? Are there any examples where you observed a stronger benefit to non-modular over modular generator functions? This could provide stronger empirical support for your theoretical contribution.\n2. Lines 422-424: For clarity, I suggest stating explicitly that you calculated the Rand index between the resulting clustering and the clustering induced by the ground-truth labels.\n3. In l.213-218, I suggest replacing \"a formal discussion on the well-definedness is lacked\", which sounds vague, with a clear and explicit statement of what you prove that the referred paper did not.\n\n## Possible errata\n\n1. Lines 213-218 and 236: The citation (Iyer & Bilmes 2012a) should probably be (Iyer & Bilmes 2012b).\n2. Line 447, the first instance of 'bar' should probably be 'shrink'.\n3. Line 701: 'and' in \"and for every\" should probably be removed.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 8,
          "confidence": 3,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "Bregman divergence is a pseudo-distance for comparing vectors or functions. In this paper, the authors present a new technique to construct such divergence on a discrete domain. Specifically:\n\n1. The authors prove that a strictly submodular function can induce a Bregman divergence (Theorem 3.1).\n2. They extend this property, showing that when the function has the form $f + m$, where $m$ is a modular function and $f$ is neither submodular nor supermodular, it can also induce a Bregman divergence (Theorem 3.1').\n3. Finally, they demonstrate that the broader the function class, the broader the class of induced divergences (Theorem 3.4).\n4. They provide a numerical form of the proposed discrete Bregman divergence (Section 4).\n5. Numerical experiments show that the learnable discrete Bregman divergences can capture structure and outperform existing methods in downstream tasks.",
          "soundness": 2,
          "presentation": 2,
          "contribution": 3,
          "strengths": "1. The authors extend previous work on constructing Bregman divergence by relaxing the requirement that the generator function $f$ must be submodular.  \n2. They present a numerical form of the proposed discrete Bregman divergence using permutation-invariant neural networks.  \n3. In the experiment section, the authors demonstrate that the constructed Bregman divergence returns smaller values for similar set pairs and larger values for dissimilar set pairs.",
          "weaknesses": "1. In Section 5.2, *Real Data Application*, it appears that there are no baselines for either the clustering or shape retrieval experiments. Figure 2 demonstrates the performance of the proposed Bregman divergence, but it is difficult to see how this new divergence improves upon previous divergences, such as those presented in [Iyer and Bilmes (2012a)].\n\nAdditionally, quantitative metrics (e.g., accuracy in the shape retrieval/classification experiment) and wall-clock comparisons to assess the performance of the proposed divergence should be included and discussed.\n\n2. The differences between the new divergence construction technique and previous work [Iyer and Bilmes (2012a)] remain unclear. For instance, in Equation (7), based on Iyer’s work, $f$ should be submodular, and $h_Y$ serves as its subgradient, which is straightforward to construct. \n\nIn the new divergence construction technique proposed here (presumably based on Theorem 3.1'), $f$ can be any set function, which raises the challenge of finding the appropriate modular mapping $h_Y$. In short, the old technique imposes a requirement on $f$, making $h_Y$ straightforward once $f$ is constructed. The new technique has no requirement for $f$ but requires finding an appropriate $h_Y$. Given this, it is not immediately clear why the new technique would outperform the previous one.",
          "questions": "1. Regarding the notation in Theorem 3.1, it appears that $h_Y$ and $g_Y$ are indeed independent of the set $Y$. Is that correct?\n\n2. In Table 2, could you clarify the values in each column? Specifically, are they the mean and variance of 10 trials of what particular measure?\n\n3. In Theorem 3.1' and the contribution section (lines 069-071), the authors mention proving that $f$ does not need to be submodular. However, in Section 4, lines 306-307, it appears that the implementation still requires submodular functions $f_1$ and $f_2$. What, then, are the advantages and differences of the new divergence construction technique compared to the original one by [Iyer and Bilmes (2012a)], which requires submodularity? It seems that the proposed technique still relies on submodular properties in the implementation stage.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 6,
          "confidence": 3,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "In this work, the authors generalize discrete Bregman divergences (DBDs) introduced in Iyer and Bilmes (2012a) to the case where the generating functions are not restricted to be submodulars. By leveraging the fact that any set function can be decomposed as the difference of two submodular ones, the authors show that any set function induces a DBD, and that this extension enables to define larger classes of DBDs. Additionally, they propose a framework to learn such divergences from observations by leveraging existing permutation invariant architectures, such as PointNet, that are by construction submodulars. While obtaining the decomposition of a set function as a difference of two submodular ones take exponential complexity, the authors propose to directly model generalized DBDs using the difference of two parametrized submodular functions obtained from PointNets. Then, they propose to learn an adapted DBD from labeled observations using the triplet loss, and show experimentally the application of their approach for clustering and retrieval tasks on two real-world datasets.",
          "soundness": 2,
          "presentation": 3,
          "contribution": 2,
          "strengths": "The paper is well-written and presented in a clear, accessible manner. The authors thoroughly acknowledge the relevant literature and prior work, thereby enhancing the clarity of their contributions. To the best of my knowledge, the proposed generalization of DBDs is novel, as is the framework introduced for learning them.",
          "weaknesses": "The main weaknesses of the paper are twofold: (1) a lack of motivation, and, relatedly, (2) a lack of empirical evidence to demonstrate the benefits of the proposed approach. Regarding the first point, while the paper introduces a new mathematical tool—the (generalized) DBD—the motivation for its introduction is not sufficiently developed. Although the authors demonstrate that their extension allows the definition of larger classes of DBDs, the motivations for considering DBDs in the first place for ML tasks are not clearly articulated. This should be clarified in the introduction and related work. Concerning the second point, the authors aim to demonstrate the applications of this tool for clustering and retrieval tasks; however, the experimental results might be insufficient to demonstrate how the proposed generalization improves upon standard DBDs using only submodular generating functions, as only one experiment is provided to demonstrate this point. Furthermore, it remains unclear how DBDs compare to other baseline methods capable of addressing similar tasks as no comparative analysis has been carried out. To enhance the impact of their work, I suggest that the authors strengthen the motivation, and compare the proposed approach with SoTA baselines on the tasks considered.",
          "questions": "How does the proposed approach compare with other baselines for clustering and retrieval tasks?\nCan the authors provide more experimental evidence on the advantage of considering their generalized DBD rather than using the standard one with submodular generating functions?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 5,
          "confidence": 3,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "The submission proposes new class of divergences on discrete spaces and a learning framework comprised of permutation-invariant neural networks with metric learning loss.\n\nLeveraging difference-of-submodular (DS) decomposition for any set function f, the authors obtain more expressive class of Bregman divergences dubbed discrete Bregman divergences (DBD). Expressiveness advantage over submodular Bregman divergences is achieved by extending the underlying set function class to not necessarily be submodular but rather admit DS decomposition.\n\nThe paper validates the proposed approach to learning DBD in a set of numerical experiments.",
          "soundness": 2,
          "presentation": 3,
          "contribution": 2,
          "strengths": "The paper addresses a problem of extending Bregman divergences to discrete spaces. This is an important problem as many methods that rely on Bregman divergences in continuous settings may be adapted to discrete scenarios.\n\nThe submission does a good job providing a structured and clear background on the treated problem.\n\nThe central idea of the paper stems from DS decomposition. Typically, one needs a submodular function $f$ to instantiate submoduler Bregman divergence. However, the authors notice that DS decomposition proposed in prior work alleviates this constraint on $f$ itself, and propose to use the difference of two submodular functions, $f = f^1 - f^2$. I'm not an expert in the area, but it seems the idea of using DS decomposition in forming more expressive class of Bregman divergences on discrete spaces has not been explored in the literature and is novel.\n\nThe authors provide empirical validation for the proposed learning framework. Given two submodular $f^1, f^2 $ functions, DS decomposition facilitates discrete Bregman divergence specification and with proper implementation of $f^1, f^2$ one can utilize metric learning approach to learn divergence from data. This defines a natural combination of permutation-invariant neural networks (to work with sets) and triplet loss (to learn the divergence). With MNIST experiment, they illustrate that the learned divergence provides reasonable results for dissimilarity between sets of images, with similar image sets getting smaller dissimilarity scores. Point cloud dataset experiments validate that the use of novel divergence class provides benefits over the use of submodular Bregman divergences in set clustering experiment. They further validate that the divergence learned provides semantically close set retrieval.",
          "weaknesses": "At the very start, the paper highlights the problem of identifiability of divergence, which comes from divergence definition $D(x, y) = 0 \\rightarrow x=y$. Thus, later the submission puts a lot of attention on the strict submodularity of the underlying set functions in Bregman divergences as one needs strict inequalities to satisfy the definition requirement. However, the proposed implementation seems problematic to me in this sense. Indeed, the submodularity is guaranteed, but DBD requires strict DS decomposition, so $f^i$s should be strictly submodular, which doesn't seem the case for the adopted architecture:\n$$\nf_{PN}([x_i]_{i \\in X}) = \\max_{i \\in X} ReLU(h(x_i)),\n$$\nwhere $h: \\mathcal{X} \\rightarrow \\mathbb{R}$ instantiated with an MLP with last activation set to ReLU. When argmax is achieved outside the intersection, the inequality from Definition 2.3 holds, but if argmax is inside the intersection, we fail to meet strict modularity. So the proposed implementation doesn't match the requirements of DBD. Perhaps I'm missing something here?\n\nSince the proposed approach targets practical side of things, it seems it needs more extensive experimentation. For example, one would expect to see expressiveness comparison that is not limited to only one task (set clustering) and one dataset (ModelNet40). This will help gain more empirical evidence for the substantial gain in expressiveness across tasks and task complexities, and justify the use of DBD which requires more computational resources (two functions instead of one).\n\nThe authors didn't discuss the limitations of their approach, which partly stems from limited experimentation.",
          "questions": "Formally, the proposed implementation is not divergence as per the Definition 1.1. But rather the generalized divergence? What are the consequences of that in practice? In what scenarios the difference may play a crucial role?\n\nAs noted in the weaknesses section, the expressiveness of the new DBD is fully explored empirically. Can we quantify how much more expressive the new class of divergences is compared to the submodular Bregman divergences? I think this is important to show the strength of the new DBD and should be explored empirically.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 8,
          "confidence": 2,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "Review of \"DISCRETE BREGMAN DIVERGENCE\" submitted to ICLR 2025.\n\nThis paper extends the submodular Bregman divergence methods introduced a few years ago to non-submodular functions. They do this via the notion of a DS (difference of submodular) decomposition of non-submodular functions. The key thing is that in order for the identifiability property of the Bregman divergence to hold (i.e., that D(x,y) = 0 iff x==y), they note that the submodular function needs to be strict (i.e., lie on the interior of the submodular cone), something they point out was not mentioned in the past (although arguably it was implicit). They note that any set function has a DS decomposition in terms of two strict submodular (or two strict supermodular) functions and since submodular functions have both sub-gradients and super-gradients, it is possible to use the two DS components of any set function to define a sub-gradient and a super-gradient based Bregman divergence, and therefore define a Bregman divergence for any set function.\n\nThey go on to show that these functions can be learnt, i.e., one can learn two submodular functions and then find the semi-gradients of these to produce a discrete Bregman divergence based on these learnt submodular functions. They show results on some set clustering that seem to me to be reasonable.\n\nWhile I do not think that the paper is revolutionary, and I do think that the strictness of previous DS decompositions was implicit, I do think it is worth pointing that out more explicitly as this paper (and also the Li & Du, 2020) in fact do, so I agree with this. There are a few issues of tone, however, that I would change in the paper and that I point out below. Also there are a few recent citations that I think you should add. All in all, however, I think the paper should be accepted as it is a nice contribution, and it is in particular good to see the empirical work using their submodular-supermodular Bregman divergence methods.\n\nHere are some comments.\n\nFirstly, I think you might consider changing the title of the paper a Submodular-Supermodular Bregman Divergence, or \"Discrete DS Bregman Divergence\" since the approach is entirely dependent on there being a DS decomposition. If one only has oracle access to a non-submodular non-supermodular set functions, it can be hard to find a reasonable decomposition (assuming one knows bounds of the function, one can always add and subtract a very large strict submodular function to any set function to transform it to a DS function but that is a fairly vacuous DS decomposition). So unless you really can produce a Bregman Divergence for any set function given only oracle access, I think it is more appropriate to entitle your paper \"Submodular-Supermodular Bregman Divergence\".\n\nI think you may want to change the tone of lines 213-216 where you say \"a formal discussion on the well-definedness is lacked\" as that sounds a bit disparaging. You are basing your results strongly on their methods, standing on their shoulders, so you might say something along the lines of \"This earlier work, however, did not explicitly mention that in order for the identifiability property of the Bregman divergence to always hold, it is necessary for the submodular functions involved to be strict\", i.e., be more explicit in what you are building on rather than saying that the previous paper \"is lacked\".\n\nI think the numerical experiments are good and, as mentioned above, it is good to see empirical work as well on submodularity, I think there should be more such things.\n\nThe submodular functions that you are learning however seem to be either deep submodular functions (DSFs), i.e., see (Bilmes & Bai from 2017, https://arxiv.org/abs/1701.08939) or much more recently deep submodular peripteral networks (DSPNs, https://arxiv.org/abs/2403.08199 from 2024). I think both papers should be cited. In particular, it seems your submodular functions are simple forms of DSPNs, but I think that one could learn two DSPNs and construct one of your Bregman divergences from semi-gradients of DSPNs quite easily, and this would both further extend the expressivity of your Bregman divergences and also extend the utility of these DSPNs. Also DSPNs strictly extend DSFs (removing the only known limitation of DSFs), and this is also useful for discrete Bregman divergences.",
          "soundness": 4,
          "presentation": 3,
          "contribution": 4,
          "strengths": "see above.",
          "weaknesses": "see above.",
          "questions": "see above.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 8,
          "confidence": 5,
          "code_of_conduct": "Yes"
        }
      ],
      "contribution": "Our contribution. In this paper, we introduce a novel class of divergences on discrete spaces that is strictly more expressive than submodular Bregman divergences and propose a learning framework of the divergences through permutation-invariant NNs. First, we formally show that the submodular-Bregman divergence is indeed a divergence if \\(f\\) is strictly submodular. By extending this observation, we further propose a new class of divergences that can be defined even if \\(f\\) is neither submodular nor supermodular. Since the concrete way to compose the divergence depends on the difference-of-submodular decomposition (Narasimhan and Bilmes, 2005; Iyer and Bilmes, 2012; Li and Du, 2020), we call it _difference-of-submodular Bregman divergences_. We show that the expressive power of difference-of-submodular Bregman divergences defined by set function \\(f\\) gets richer if the class of the underlying set function \\(f\\) gets larger. We then propose a learnable form of difference-of-submodular Bregman divergences based on submodular permutation-invariant NNs. Numerical experiment shows that learnable difference-of-submodular Bregman divergences can capture the crucial structure and significantly improves the performance of existing methods in downstream tasks.",
      "paper_id": "vr1QdCNJmN",
      "pdf": "/pdf/7833fafa72584d99f69d14f23bac61113c2d6ae2.pdf",
      "venue": "ICLR 2025 Poster",
      "summary": {
        "introduction": "The paper discusses the concept of divergence functions that measure dissimilarity in spaces, emphasizing the classical Bregman divergence which is generated from convex functions suitable for continuous spaces. It highlights the difficulties in creating analogous divergence measures for discrete spaces, particularly for set data. The authors introduce the idea of using submodular functions as generating functions for Bregman-like divergences in discrete domains, citing prior work by Iyer & Bilmes (2012b). They identify limitations of existing submodular Bregman divergences, such as issues with identifiability and the restrictive nature of submodular functions, motivating the need for a more flexible framework. The main contribution is the proposal of a new divergence called the difference-of-submodular Bregman divergence, which allows broader classes of set functions, including those neither submodular nor supermodular, and introduces a learnable neural network-based approach. This work aims to provide a useful tool for tasks involving structured discrete data, such as clustering and set retrieval.",
        "related_work": "The related work section reviews the significance of Bregman divergences across various machine learning and statistical areas, including clustering (e.g., kernel k-means), relationship with exponential family distributions, information geometry, matrix factorization, optimization algorithms like mirror descent, and variational inference. It connects these continuous-space applications to the challenge of defining similar measures on discrete sets. The work on metric learning and neural network-based representation learning is discussed, emphasizing that prior literature has not addressed learning Bregman divergences on discrete spaces. The section underscores the importance of design flexibility and expressiveness in divergence measures and positions the current contribution as a novel extension to the existing literature by integrating submodular and non-submodular set functions with neural network learning.",
        "background": "The background lays foundational concepts necessary for understanding the proposed divergence. It introduces submodular functions, their properties, and their relationship to convex functions, including subdifferentials and superdifferentials. It defines strict submodularity, important for ensuring divergence properties like identifiability, with examples such as facility location and log-sum-exp relaxations. The section describes permutation-invariant neural networks, such as PointNet and related architectures, which are essential for modeling set-structured data. The mathematical form of these networks and their properties are discussed, establishing the tools used later for learnable divergence modeling. This section provides the theoretical underpinnings for combining set functions with neural networks to construct flexible, structure-preserving distance metrics in discrete spaces.",
        "method": "The methodology introduces the generalized Bregman divergence for discrete set functions, emphasizing the role of strict submodularity for ensuring divergence properties. It presents the concept of decomposing a general set function into a difference of two monotone, strictly submodular functions (the strong DS decomposition), which allows defining divergence measures even for non-submodular functions, now called difference-of-submodular Bregman divergences (DSBD). The authors prove that any set function can be decomposed into such a form, expanding the class of functions usable for divergence construction. They propose a learning framework utilizing permutation-invariant neural networks, specifically epsilon-PointNet architectures, to learn these set functions. The neural networks are trained using a triplet loss to embed the set data effectively. The approach allows the divergence to adapt to specific tasks and data structures, providing a flexible, learnable, and structure-preserving measure of dissimilarity in discrete domains.",
        "experiments": "The experimental section evaluates the proposed divergence on synthetic and real-world data. An illustrative experiment on MNIST demonstrates that the learned divergence behaves as a proper dissimilarity measure, assigning smaller values to similar set pairs and larger to dissimilar ones. The real-data application involves set clustering and retrieval on the ModelNet40 point cloud dataset, where sets are point clouds of objects. The neural network models are trained using triplet loss, with different configurations of submodular and non-submodular functions. Quantitative results show that the learned divergence outperforms fixed submodular Bregman divergences in clustering accuracy, as measured by Rand index. The importance of the difference-of-submodular decomposition and the choice of supergradients is analyzed, with findings indicating that the decomposition enhances performance and stability. Qualitative set retrieval experiments further demonstrate the divergence’s capacity to identify similar object sets, leading to retrieval results consistent with object categories.",
        "results": "The results show that the proposed learnable difference-of-submodular Bregman divergence effectively captures structural relationships in discrete data. Numerical experiments on MNIST illustrate its conceptual correctness by reflecting expected dissimilarities. On the ModelNet40 dataset, the learned divergence significantly improves clustering performance over traditional submodular-based divergences, notably when the DS decomposition is used. The approach with the DS decomposition yields lower variability in results, indicating stability. The divergence also performs well in set retrieval tasks, closely matching the human-expected similarity in object classes, and outperforms previous methods in quantitative scores. These empirical findings support the theoretical claim that expanding the class of set functions enhances the divergence's expressiveness and task-specific performance.",
        "discussion": "The discussion underscores the significance of extending Bregman divergences to discrete set data, highlighting the theoretical and empirical advantages of the difference-of-submodular approach. It emphasizes that the proposed framework broadens the class of functions that can generate meaningful divergence measures, leading to more expressive and task-adaptive metrics. The experimental results validate the benefits of the DS decomposition and neural network learning, especially in complex structures like point clouds. The authors acknowledge limitations related to computational complexity of the DS decomposition and the choice of neural network architectures. Future directions include exploring more advanced neural models for set functions and further optimizing the decomposition process to enhance practical applicability. The work contributes a new paradigm for discrete structured data analysis, with promising avenues for personalized, data-driven divergence design.",
        "conclusion": "The conclusion summarizes that traditional Bregman divergences, rooted in convex functions for continuous spaces, have been successfully extended to finite discrete sets through the proposed difference-of-submodular Bregman divergence framework. This approach enables the use of general set functions, beyond submodular ones, by leveraging the strong DS decomposition. Theoretical analysis confirms that richer set functions lead to more expressive divergence measures. The neural network-based learning method effectively adapts these divergences to specific tasks, demonstrated by improved clustering and retrieval in experiments. The study emphasizes that this development opens new possibilities for structure-aware dissimilarity measures in discrete data, with potential applications across machine learning tasks involving complex structured data."
      }
    },
    {
      "title": "GeoILP: A Synthetic Dataset to Guide Large-Scale Rule Induction",
      "abstract": "Inductive logic programming (ILP) is a machine learning approach aiming to learn explanatory rules from data.\n    While existing ILP systems can successfully solve small-scale tasks, large-scale applications with various language biases are rarely explored.\n    Besides, it is crucial for a large majority of current ILP systems to require expert-defined language bias, which hampers the development of ILP towards broader utilizations.\n    In this paper, we introduce GeoILP, a large-scale synthetic dataset of diverse ILP tasks involving numerous aspects of language bias.\n    % including complex rule forms, high deduction complexity, and more realistic assumptions.\n    The ILP tasks are built from geometry problems, at the level from textbook exercise to regional International Mathematical Olympiad (IMO), with the help of a deduction engine.\n    These problems are elaborately selected to cover all challenging language biases, such as recursion, predicate invention, and high arity.\n    Experimental results show that no existing method can solve GeoILP tasks.\n    In addition, along with classic symbolic-form data, we provide image-form data to boost the development of the joint learning of neural perception and symbolic rule induction.",
      "citationCount": -1,
      "reviews": [
        {
          "review": "The paper builds a novel ILP dataset to boost the development of the ILP community development. The proposed ILP is very challenging based on the statements of the paper. In addition, the authors present the methodology for generating the proposed ILP dataset.1. Based on the structure of the paper, only Section 5 describes the proposed GeoILP. The rest of the Sections look like a survey to describe the development of the ILP methodology. Hence, the contribution of the paper including the method to generate the datasets and the evaluation of the proposed dataset for proving some properties such as learning recursive rules and long variables rules with the existing ILP models is still limited. \n2. When learning from raw data, the authors only discussed learning rules with the help of a pre-trained perception model. However, some discussions about learning from raw data directly without the symbolization process by the perception model are missing.",
          "rating": 3
        },
        {
          "review": "- The paper discussed the challenges for the current ILP area in detail, and the authors clearly understood the intrinsic disadvantages of ILP, thus the dataset is designed to motivate the community to resolve these long-neglected issues.\n- The authors have covered extensive related works, and the paper is well structured and well written.- The design of the dataset is thoughtful, however, it is still like the previous ILP tasks, which are not very accessible to the ICLR community. For example, the representation of Logic Programming or Prolog is not user-friendly for normal users. The definition and theorems in plane geometry described with logic programs in this paper sometimes are difficult to understand.\n- Prolog is not a popular language for theorem proving, modern machine learning techniques such as LLMs could not handle them well enough.\n- The experiments in this paper are not enough, it is only experimented with Popper.",
          "rating": 8
        },
        {
          "review": "1. The paper is clearly written.\n\n2. The dataset GEOILP is  large-scale and mimics real-world data better than traditional closed-world datasets.The article provides a lot of background information, which results in insufficient coverage of its own work. \nFirstly, the experimental section is difficult to support the contributions of this paper. \nAlso, although Section 5 offers a detailed introduction to the content of its dataset, it does not effectively highlight the characteristics of its dataset in comparison to other datasets.",
          "rating": 5
        },
        {
          "review": "- This paper provides a reference dataset for investigating a number of important challenges in ILP for which no current datasets exist.\n- The problem domain is intuitive, providing the potential to be expanded by others as additional points of interest arise. \n- Open challenges in ILP are well articulated, and the role of this dataset in providing a means to evaluate future developments in the field is conveyed clearly.- The predicate arity section may not be ideally motivated.  For example, the (teacher, subject, student) relationship would probably be addressed in practice by making the subject a predicate, since a relatively small set of subjects at that level of granularity exist.  While the authors' observation is valid, it but might not be compelling to someone unfamiliar with the nuance.\n- In addition to combinatorial techniques referenced heavily in the writeup, there are a wide variety of published neuro symbolic techniques.  One of the few highlighted in this paper is by Evans and Grefenstette.  Evans et. all discuss raw data challenges [1], and his dissertation [2] has extensive discussion of challenges and approach. \n\n[1] Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22)\n[2] Kant's Cognitive Architecture",
          "rating": 8
        }
      ],
      "meta_review": "This paper is truly in the borderline with varied reviews. The reviewers liked the idea of automated solutions in ILP, something that is needed sorely if the field has to be sustained in the current era of AI. From that perspective, this is an important paper with an important dataset for understanding the limitations of ILP methods. \n\nThe unfavorable opinions stem from the fact that the paper did not properly motivate the use of high-arity predicates when simple decompositions seem to work well in practice. The notion was that the paper created a circular argument by creating problems that are beyond the current systems and then claiming to solve them. It is unclear if there are much practical use of this (again, this goes back to lack of motivation).\n\nFrom my own reading of the paper and assessment, I find the data set and the solution interesting. The paper can improve its motivation better and I hope the authors consider the reviews seriously when submitting the final version.\nThere was reasonable discussion with the reviewer with negative score keeping their score (I am not sure I fully agree with the decision to keep the score after the authors had addressed several of their claims).",
      "replies": [
        {
          "summary": "This paper proposed a new dataset in the geometry domain to help the researcher enhance the inductive logic programming (ILP) models. The authors indicated that there is no reference hypothesis in the current ILP datasets. This paper proposes a larger-scale ILP dataset with reference hypotheses. This paper also describes the algorithm for how to generate the synthesis data and the corresponding reference. \n\nHowever, the paper is borderline rejected for the following reasons: (1) The motivation of the paper is not explicitly discussed in the paper. (2) The Experiments are not explicitly investigated in the paper. Hence, the overall contribution of the paper is limited. (3) Some terminology is vague to use such as learning from raw data and open-world assumption.",
          "soundness": 2,
          "presentation": 2,
          "contribution": 2,
          "strengths": "The paper builds a novel ILP dataset to boost the development of the ILP community development. The proposed ILP is very challenging based on the statements of the paper. In addition, the authors present the methodology for generating the proposed ILP dataset.",
          "weaknesses": "1. Based on the structure of the paper, only Section 5 describes the proposed GeoILP. The rest of the Sections look like a survey to describe the development of the ILP methodology. Hence, the contribution of the paper including the method to generate the datasets and the evaluation of the proposed dataset for proving some properties such as learning recursive rules and long variables rules with the existing ILP models is still limited. \n2. When learning from raw data, the authors only discussed learning rules with the help of a pre-trained perception model. However, some discussions about learning from raw data directly without the symbolization process by the perception model are missing.",
          "questions": "1. Why is this dataset helpful in solving the ILP problem? \n    1. Having reference hypotheses to guide the evaluation of the ILP model is not essential in all senses. In some ILP datasets proposed by [1] or FB15KSelected, the knowledge graph is easy to understand. Hence, there is no need to have a reference in addition. \n    2. Besides, some ILP models support precision and recall as quantitative metrics to evaluate the learned rules from data [3]. \n    3. Furthermore, when generating a set of rules, how to evaluate the performance of an ILP model based on the proposed reference hypotheses. Is the reference complete based on the background knowledge, positive examples, and negative examples?\n2. Based on these proposed datasets, the authors mentioned that no one ILP model can successfully learn rules from the GeoILP. The results are further explained in Section 6.2. The symbolic ILP models can not learn even one rule in the *basic level* setting. However, there is no explicit explanation about the basic level in line 520 page 10.  In addition, in line 524 on page 10, the authors stated that three neuro-symbolic models cannot solve the GeoILP because of the features of these ILP models. However, some neuro-symbolic ILP models can learn rules with three or more body atoms and any arities of a predicate [2]. The authors should also analyze more ILP models in Experiments to investigate the current performance of the ILP models.\n3. In addition, there is no reference in the Open-world assumption paragraph of Section  4.2. In addition, the open-world assumption is not clearly explained as to why the open-world assumption is related to intentional and extensional predicates in line 428 on page 8. The knowledge base uses the open-world assumption to determine the Boolean value of a ground atom, which is defined in line 257 on page 5. However, in line 428, the open-world assumption is applied based on the rule format. Hence, the authors should explain more about the connections between open-world assumptions and the format of rules. \n\nReference:\n[1] Richard Evans, Edward Grefenstette: Learning Explanatory Rules from Noisy Data. J. Artif. Intell. Res. 61: 1-64 (2018)\n[2] Xujie Si, Mukund Raghothaman, Kihong Heo, Mayur Naik: Synthesizing Datalog Programs using Numerical Relaxation. IJCAI 2019: 6117-6124\n[3] Tim Rocktäschel, Sebastian Riedel: End-to-end Differentiable Proving. NIPS 2017: 3788-3800",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 3,
          "confidence": 2,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "This paper presents a novel Inductive Logic Programming (ILP) dataset to evaluate ILP algorithms and systems. The presented GeoILP tasks try to formulate geometrical background knowledge as logic programs, and the motivation is to learn geometrical theories/concepts from them. Moreover, the authors also visualised all the tasks and produced images for the problems. The difficulty of the tasks is varied, ranging from simple questions to IMO-level questions.",
          "soundness": 3,
          "presentation": 3,
          "contribution": 4,
          "strengths": "- The paper discussed the challenges for the current ILP area in detail, and the authors clearly understood the intrinsic disadvantages of ILP, thus the dataset is designed to motivate the community to resolve these long-neglected issues.\n- The authors have covered extensive related works, and the paper is well structured and well written.",
          "weaknesses": "- The design of the dataset is thoughtful, however, it is still like the previous ILP tasks, which are not very accessible to the ICLR community. For example, the representation of Logic Programming or Prolog is not user-friendly for normal users. The definition and theorems in plane geometry described with logic programs in this paper sometimes are difficult to understand.\n- Prolog is not a popular language for theorem proving, modern machine learning techniques such as LLMs could not handle them well enough.\n- The experiments in this paper are not enough, it is only experimented with Popper.",
          "questions": "- Induction will be a very challenging problem for LLMs, would it be possible to extend the dataset and make a natural language version, so that people can compare ILP with LLMs?\n- Why not use formal languages that are designed specifically for automated proof, such as lean4 or Coq?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 8,
          "confidence": 4,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "The paper  introduce GeoILP, a large-scale synthetic dataset of diverse ILP tasks involving numerous aspects of language bias.\nThe dataset consists of geometry problems modeled from levels as varied as textbook exercises, promoting the development of methods that can handle complex language biases, higher arity, and multi-task learning.",
          "soundness": 2,
          "presentation": 3,
          "contribution": 3,
          "strengths": "1. The paper is clearly written.\n\n2. The dataset GEOILP is  large-scale and mimics real-world data better than traditional closed-world datasets.",
          "weaknesses": "The article provides a lot of background information, which results in insufficient coverage of its own work. \nFirstly, the experimental section is difficult to support the contributions of this paper. \nAlso, although Section 5 offers a detailed introduction to the content of its dataset, it does not effectively highlight the characteristics of its dataset in comparison to other datasets.",
          "questions": "How strong is the generalization capability of the neural-symbolic model trained on this dataset? Can it solve tasks beyond geometric problems?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 5,
          "confidence": 3,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "This paper introduces a large-scale synthetic dataset for inductive logic programming involving a number of challenging predicate language constructions.  The subject domain of geometry provides a rich set of predicates with symmetries, recursion, and constraints. Furthermore, when higher dimensional connective objects (e.g. line segments) are excluded from the background knowledge, predicates become black-box functions of the remaining universe of objects (e.g. points).  This setup provides an instructive setting in which to illustrate and investigate many prominent and unsolved challenges in ILP.",
          "soundness": 4,
          "presentation": 4,
          "contribution": 3,
          "strengths": "- This paper provides a reference dataset for investigating a number of important challenges in ILP for which no current datasets exist.\n- The problem domain is intuitive, providing the potential to be expanded by others as additional points of interest arise. \n- Open challenges in ILP are well articulated, and the role of this dataset in providing a means to evaluate future developments in the field is conveyed clearly.",
          "weaknesses": "- The predicate arity section may not be ideally motivated.  For example, the (teacher, subject, student) relationship would probably be addressed in practice by making the subject a predicate, since a relatively small set of subjects at that level of granularity exist.  While the authors' observation is valid, it but might not be compelling to someone unfamiliar with the nuance.\n- In addition to combinatorial techniques referenced heavily in the writeup, there are a wide variety of published neuro symbolic techniques.  One of the few highlighted in this paper is by Evans and Grefenstette.  Evans et. all discuss raw data challenges [1], and his dissertation [2] has extensive discussion of challenges and approach. \n\n[1] Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22)\n[2] Kant's Cognitive Architecture",
          "questions": "The statement \"Enabling recursion is expensive for symbolic ILP, while neuro-symbolic ILP does not support mutual recursion\" cites a singe neuro-symbolic method that presumably does not support mutual recursion.  Is there a theorem that says that no neuro-symbolic method can support mutual recursion?  \n\nIs there a reason to exclude the wider neuro-symbolic techniques that have been explored?\n\nCan you discuss the limitations of binary predicates in more detail?  I think this deserves more attention.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 8,
          "confidence": 3,
          "code_of_conduct": "Yes"
        }
      ],
      "contribution": "Our goal is to construct a large-scale ILP dataset, providing reference hypotheses to guide the resolution of present limitations in ILP, which would lead ILP to an expert-free learning paradigm (like modern neural learning) and exceedingly broader utilization. Furthermore, we aim to evaluate ILP systems without much expert priors (like other ML tasks), i.e., training & testing without excessive user-defined bias. Therefore, we construct GeoILP, a large-scale dataset synthesized from plane geometry rules that help generate reference hypotheses involving various language biases.",
      "paper_id": "cfGpIcOIa5",
      "pdf": "/pdf/83658b5c6df091c7f3fee850ee73527ad61590ee.pdf",
      "venue": "ICLR 2025 Poster",
      "summary": {
        "introduction": "The paper introduces GeoILP, a large-scale synthetic dataset crafted to guide large-scale rule induction in inductive logic programming (ILP). It emphasizes the importance of moving beyond small datasets and expert-defined biases, aiming to facilitate the development of ILP systems capable of automatic hypothesis discovery without extensive human prior. GeoILP is built from geometry problems, ranging from textbook exercises to IMO-level challenges, covering various language biases such as recursion, predicate invention, and high arity.",
        "related_work": "The related work section discusses existing ILP methods, including symbolic search-based systems like FOIL, Progol, ALEPH, Metagol, and Popper, which are limited by hypothesis space complexity and often require human expertise. It also covers neural-symbolic approaches that relax the hypothesis space for gradient optimization but are limited in scalability and support for language biases like high arity, recursion, and predicate invention. The section highlights existing datasets, which are either real-world with limited reference hypotheses or small-scale synthetic datasets, and notes the lack of large-scale, comprehensive ILP datasets with reference hypotheses for guiding system development.",
        "background": "The background section covers logic preliminaries such as first-order Horn clauses, definitions of positive and negative examples, and the notion of hypotheses in ILP. It describes Horn clauses, facts, rules, and the process of forward chaining for deduction, emphasizing the importance of logical entailment. The section also formalizes the ILP setting, defining the goal of inducing hypotheses from background knowledge, positive and negative examples, ensuring they entail all positives and exclude negatives. It clarifies the use of entailment-based learning, predicates, and rule structures, providing foundational concepts necessary for understanding ILP challenges and the dataset design.",
        "method": "The method involves synthesizing ILP tasks by starting with predefined rules and premises, using deduction engines to generate conclusions (examples), and traceback procedures to identify relevant background knowledge. GeoILP uses a deductive database for automated theorem proving in geometry, deriving conclusions through forward chaining on a selected set of geometric premises. The traceback step traces dependencies back from conclusions to premises to form background knowledge. The dataset features involve complex rules with high arity, recursion, predicate invention, and argument symmetry, supporting noisy and multi-task settings. It also extends to raw sensory data by generating geometric diagrams as images, thus enabling joint perception and rule induction learning. The dataset is designed to cover the limitations of existing ILP systems by increasing complexity and providing reference hypotheses.",
        "experiments": "Experiments are conducted across four progressive levels of difficulty (basic, simple, advanced, complex) for single tasks, and also include multi-task scenarios. The symbolic baseline uses Popper, which fails to solve GeoILP within reasonable time due to its complexity. Neuro-symbolic methods like Difflog either crash or run out of memory even at the basic level. The experiments highlight the significant gap between existing ILP methods and GeoILP's challenges, emphasizing that current systems are unsuitable for large-scale, complex ILP tasks. The experimental setup tests the scalability and limitations of symbolic and neuro-symbolic ILP approaches in the face of GeoILP's complexity.",
        "results": "The results show that none of the existing ILP methods, symbolic or neuro-symbolic, can effectively solve GeoILP tasks. Popper, a symbolic ILP system, fails to produce hypotheses within the given time constraints. Neuro-symbolic methods also face severe issues, such as memory overload or crashes, even at lower difficulty levels. These outcomes underscore the intrinsic difficulty of GeoILP and demonstrate that current ILP systems are not yet capable of handling such large-scale, complex problems. The experiments reinforce the need for new approaches and dataset benchmarks to push the development of scalable ILP solutions.",
        "discussion": "The discussion emphasizes that GeoILP is a significantly more challenging benchmark compared to existing datasets, with its large hypothesis size, high complexity, and comprehensive language biases. It demonstrates that current symbolic ILP systems are inadequate for such tasks, and even advanced neuro-symbolic methods struggle with memory and scalability issues. The authors suggest that GeoILP exposes critical bottlenecks in ILP research, including the handling of high-arity predicates, recursion, predicate invention, noisy data, and raw inputs. The work advocates for future research to develop more scalable, flexible, and perceptually integrated ILP systems. The conclusion highlights that GeoILP provides a rigorous testbed to advance ILP toward broader and more autonomous applications.",
        "conclusion": "The paper concludes by presenting GeoILP as a comprehensive, large-scale synthetic dataset that encapsulates core challenges in ILP, such as language bias, recursion, predicate invention, and noisy data, across numerous tasks in geometry. It serves as a vital benchmark for evaluating and driving the progress of ILP systems towards automatic, scalable, and neural-symbolic rule learning. The authors note that existing ILP systems cannot handle GeoILP's complexity, revealing the necessity for novel algorithms and models that can operate effectively at this scale. The dataset also uniquely includes raw sensory inputs, fostering joint perception and reasoning research, making GeoILP a key step towards more general and robust AI."
      }
    },
    {
      "title": "Binary Reward Labeling: Bridging Offline Preference and Reward-Based Reinforcement Learning",
      "abstract": "Offline reinforcement learning has become one of the most practical RL settings. However, most existing works on offline RL focus on the standard setting with scalar reward feedback. It remains unknown how to universally transfer the existing rich understanding of offline RL from the reward-based to the preference-based setting. In this work, we propose a general framework to bridge this gap. Our key insight is transforming preference feedback to scalar rewards via binary reward labeling (BRL), and then any reward-based offline RL algorithms can be applied to the dataset with the reward labels. The information loss during the feedback signal transition is minimized with binary reward labeling in the practical learning scenarios. We theoretically show the connection between several recent PBRL techniques and our framework combined with specific offline RL algorithms. By combining reward labeling with different algorithms, our framework can lead to new and potentially more efficient offline PBRL algorithms. We empirically test our framework on preference datasets based on the standard D4RL benchmark. When combined with a variety of efficient reward-based offline RL algorithms, the learning result achieved under our framework is comparable to training the same algorithm on the dataset with actual rewards in many cases and better than the recent PBRL baselines in most cases.",
      "citationCount": -1,
      "reviews": [
        {
          "review": "1) Theoretical: In the case of non-overlapping trajectories, the relationship between the binary-encoding-based reward model and the traditional reward model is established.\n\n2) Experimental: The performance of the algorithm is simulated under both overlapping and non-overlapping trajectory scenarios.1) Writing: The sections on related work and theoretical foundations are overly redundant. Some statements, particularly in the introduction, are inaccurately expressed. For example, current offline PbRL methods primarily focus on reward model learning, rather than on the policy learning aspect itself. For example, in lines 47-49 and 72-76 of the paper.\n\n2) Motivation: The motivation of the paper is unclear. The authors state that the main goal is to develop a framework to bridge the gap between PbRL and standard RL, allowing a standard offline RL algorithm to address the PbRL problem. However, the primary motivation behind PbRL is to resolve the challenge of setting rewards in standard RL. The difficulty in PbRL lies in accurately learning rewards from human preferences, which is not a problem that standard offline RL addresses. The author could approach this from the perspective of overlapping (or similar) trajectories and inconsistent labels, which might lead to a more effective explanation.\n\n3) Theory: Theoretical 4.5 only considers the case of non-overlapping trajectories and does not account for the scenario of overlapping trajectories with inconsistent labels.\n\n4) Experiments: The dataset is limited, with experiments conducted solely in the mujoco tasks. The paper does not compare results with cutting-edge PbRL methods, such as PT ( Preference transformer: Modeling human preferences using transformers for rl).",
          "rating": 3
        },
        {
          "review": "1. The problem of reward labeling from preference labels is a fundamental challenge in offline PBRL.\n2. The performance improvement is impressive.1. Presentation is poor. The citations are poorly formatted and hard to read.\n2. Lack of discussion about comparison against the commonly used BT reward model. The contribution is poorly justified.\n3. The authors claim that \"For the baseline methods, to the best of our knowledge, no existing empirical study works in exactly\nthe standard offline PBRL setting considered in our work\". However, there have been massive studies on offline preference-based RL, such as PreferenceTransformer (https://arxiv.org/pdf/2303.00957) and OPRL (https://arxiv.org/pdf/2301.01392) and can be readily adopted into the experiment framework.\n4. (https://proceedings.neurips.cc/paper_files/paper/2023/file/c3e969ea20542a6a11e6caeac736a0b9-Paper-Conference.pdf) reveals that D4RL tasks are not sensitive to reward labels. So the empirical results may not be convincing.",
          "rating": 3
        },
        {
          "review": "This work investigate an important problem and conduct the theoretical analysis for the method.1. The writing of this work is not good. I get confused for many spaces. What is link function? What is link-loss function? The writing of Section 4.1 is very confusing and incomprehensible. The pseudocode is too concise.\n\n2. Missing a lot of baseline algorithms. For example, OPRL [1] and PT [2].\n\n[1] Shin, Daniel, Anca D. Dragan, and Daniel S. Brown. \"Benchmarks and algorithms for offline preference-based reward learning.\" arXiv preprint arXiv:2301.01392 (2023).\n\n[2] Kim, Changyeon, et al. \"Preference transformer: Modeling human preferences using transformers for rl.\" arXiv preprint arXiv:2303.00957 (2023).",
          "rating": 3
        },
        {
          "review": "- The paper introduces a simple and unique method for translating preference feedback into a format that can be used by standard offline RL algorithms, which is a significant step forward in the field of PBRL.\n\n- The authors provide a theoretical analysis that connects their framework with existing PBRL techniques, providing an interesting point of view and adding depth to the understanding of how preference information can be utilized in RL.- The paper suffers from poor writing quality and formatting issues, which detract from the overall presentation and readability. For example, in Definition 4.2, there should be a period after \"reward modeling in model-based approaches,\" and the comma should not appear at the start of a line. The subtitle \"Offline standard RL algorithms are model-based.\" in Section 4.2 can be misleading.\n\n- The soundness of the proposed method is questionable. While the $\\pm 1$ reward labeling is theoretically correct, it is usually not a good choice to overfit the preference dataset. Having a more rigorous analysis under the function approximation scenario would be nice.\n\n- The paper needs some benchmarks and baselines to validate the effectiveness of the proposed method. For benchmarks, The D4RL benchmark is known to be insensitive to the accuracy of the reward function [1], and adding benchmarks like Meta-World would greatly strengthen the paper. Also, there are some recent works on offline PbRL that have a strong performance, like [2,3], and BRL should be compared with them.\n\n\nReferences\n\n[1] Li, Anqi, et al. \"Survival instinct in offline reinforcement learning.\" Advances in neural information processing systems 36 (2024).\n\n[2] Kim, Changyeon, et al. \"Preference transformer: Modeling human preferences using transformers for rl.\" arXiv preprint arXiv:2303.00957 (2023).\n\n[3] Zhang, Zhilong, et al. \"Flow to better: Offline preference-based reinforcement learning via preferred trajectory generation.\" The Twelfth International Conference on Learning Representations. 2023.",
          "rating": 5
        }
      ],
      "meta_review": "Not Given",
      "replies": [
        {
          "summary": "This paper proposes a binary-encoding-based reward model learning method for preference-based reinforcement learning. The method demonstrates superior performance in both overlapping and non-overlapping trajectory scenarios.",
          "soundness": 3,
          "presentation": 2,
          "contribution": 3,
          "strengths": "1) Theoretical: In the case of non-overlapping trajectories, the relationship between the binary-encoding-based reward model and the traditional reward model is established.\n\n2) Experimental: The performance of the algorithm is simulated under both overlapping and non-overlapping trajectory scenarios.",
          "weaknesses": "1) Writing: The sections on related work and theoretical foundations are overly redundant. Some statements, particularly in the introduction, are inaccurately expressed. For example, current offline PbRL methods primarily focus on reward model learning, rather than on the policy learning aspect itself. For example, in lines 47-49 and 72-76 of the paper.\n\n2) Motivation: The motivation of the paper is unclear. The authors state that the main goal is to develop a framework to bridge the gap between PbRL and standard RL, allowing a standard offline RL algorithm to address the PbRL problem. However, the primary motivation behind PbRL is to resolve the challenge of setting rewards in standard RL. The difficulty in PbRL lies in accurately learning rewards from human preferences, which is not a problem that standard offline RL addresses. The author could approach this from the perspective of overlapping (or similar) trajectories and inconsistent labels, which might lead to a more effective explanation.\n\n3) Theory: Theoretical 4.5 only considers the case of non-overlapping trajectories and does not account for the scenario of overlapping trajectories with inconsistent labels.\n\n4) Experiments: The dataset is limited, with experiments conducted solely in the mujoco tasks. The paper does not compare results with cutting-edge PbRL methods, such as PT ( Preference transformer: Modeling human preferences using transformers for rl).",
          "questions": "1) Please authors further clarify the motivation of this paper. (This is the main question)\n\n2) How does the algorithm perform in cases where trajectories overlap and labels are inconsistent? The author could discuss how their theoretical results might extend to or be limited by scenarios with overlapping trajectories.\n\n3) What are the advantages of the binary-encoding-based reward model compared to the traditional reward model?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 3,
          "confidence": 3,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "This paper discusses the problem of acquiring a reward function from offline preference datasets. The authors claim that binary reward labelling is sufficient for solving this problem. Results on D4RL demonstrate the effectiveness of the proposed method.",
          "soundness": 2,
          "presentation": 1,
          "contribution": 2,
          "strengths": "1. The problem of reward labeling from preference labels is a fundamental challenge in offline PBRL.\n2. The performance improvement is impressive.",
          "weaknesses": "1. Presentation is poor. The citations are poorly formatted and hard to read.\n2. Lack of discussion about comparison against the commonly used BT reward model. The contribution is poorly justified.\n3. The authors claim that \"For the baseline methods, to the best of our knowledge, no existing empirical study works in exactly\nthe standard offline PBRL setting considered in our work\". However, there have been massive studies on offline preference-based RL, such as PreferenceTransformer (https://arxiv.org/pdf/2303.00957) and OPRL (https://arxiv.org/pdf/2301.01392) and can be readily adopted into the experiment framework.\n4. (https://proceedings.neurips.cc/paper_files/paper/2023/file/c3e969ea20542a6a11e6caeac736a0b9-Paper-Conference.pdf) reveals that D4RL tasks are not sensitive to reward labels. So the empirical results may not be convincing.",
          "questions": "1. Why does the binary reward outperform BT model? Will the empirical results still hold in more complex tasks such as Meta-World?\n2. How do baseline methods such as Preference Transformer perform on the benchmarks?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 3,
          "confidence": 2,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "This manuscript introduces a novel framework aimed at addressing the challenge of transferring knowledge from reward-based to preference-based offline reinforcement learning (PBRL). The authors highlight that while offline RL has gained practical significance, most research has been limited to scalar reward feedback, leaving a gap in understanding how to apply offline RL techniques to preference-based settings. The proposed solution involves converting preference feedback into scalar rewards through binary reward labeling (BRL), which allows the application of any reward-based offline RL algorithms to datasets with these labels. This approach minimizes information loss during the transition from preference to scalar rewards. The paper establishes theoretical connections between recent PBRL techniques and the proposed framework when combined with specific offline RL algorithms, suggesting that the framework can yield new and more efficient offline PBRL algorithms. Empirical tests on preference datasets from the D4RL benchmark demonstrate that the framework's performance, when combined with various efficient reward-based offline RL algorithms, is often comparable to training on datasets with actual rewards and superior to recent PBRL baselines in most cases.",
          "soundness": 2,
          "presentation": 2,
          "contribution": 2,
          "strengths": "This work investigate an important problem and conduct the theoretical analysis for the method.",
          "weaknesses": "1. The writing of this work is not good. I get confused for many spaces. What is link function? What is link-loss function? The writing of Section 4.1 is very confusing and incomprehensible. The pseudocode is too concise.\n\n2. Missing a lot of baseline algorithms. For example, OPRL [1] and PT [2].\n\n[1] Shin, Daniel, Anca D. Dragan, and Daniel S. Brown. \"Benchmarks and algorithms for offline preference-based reward learning.\" arXiv preprint arXiv:2301.01392 (2023).\n\n[2] Kim, Changyeon, et al. \"Preference transformer: Modeling human preferences using transformers for rl.\" arXiv preprint arXiv:2303.00957 (2023).",
          "questions": "1. Can you evaluate your algorithms on various domains? For example, Antmaze, Kitichen and Adroit?",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 3,
          "confidence": 4,
          "code_of_conduct": "Yes"
        },
        {
          "summary": "The paper presents a novel framework aimed at bridging the gap between offline preference-based reinforcement learning (PBRL) and standard offline reward-based reinforcement learning (RL). The authors propose a method called Binary Reward Labeling (BRL), which transforms preference feedback into scalar rewards, allowing the application of any reward-based offline RL algorithm to datasets with reward labels. The key insight is simply relabel the reward function with $\\pm 1$ using preference labels. The paper provides theoretical connections between PBRL techniques and the proposed framework combined with specific offline RL algorithms. Empirical tests on preference datasets based on the D4RL benchmark demonstrate that the framework's performance is comparable to training on datasets with actual rewards and superior to recent PBRL baselines in many cases.",
          "soundness": 2,
          "presentation": 2,
          "contribution": 2,
          "strengths": "- The paper introduces a simple and unique method for translating preference feedback into a format that can be used by standard offline RL algorithms, which is a significant step forward in the field of PBRL.\n\n- The authors provide a theoretical analysis that connects their framework with existing PBRL techniques, providing an interesting point of view and adding depth to the understanding of how preference information can be utilized in RL.",
          "weaknesses": "- The paper suffers from poor writing quality and formatting issues, which detract from the overall presentation and readability. For example, in Definition 4.2, there should be a period after \"reward modeling in model-based approaches,\" and the comma should not appear at the start of a line. The subtitle \"Offline standard RL algorithms are model-based.\" in Section 4.2 can be misleading.\n\n- The soundness of the proposed method is questionable. While the $\\pm 1$ reward labeling is theoretically correct, it is usually not a good choice to overfit the preference dataset. Having a more rigorous analysis under the function approximation scenario would be nice.\n\n- The paper needs some benchmarks and baselines to validate the effectiveness of the proposed method. For benchmarks, The D4RL benchmark is known to be insensitive to the accuracy of the reward function [1], and adding benchmarks like Meta-World would greatly strengthen the paper. Also, there are some recent works on offline PbRL that have a strong performance, like [2,3], and BRL should be compared with them.\n\n\nReferences\n\n[1] Li, Anqi, et al. \"Survival instinct in offline reinforcement learning.\" Advances in neural information processing systems 36 (2024).\n\n[2] Kim, Changyeon, et al. \"Preference transformer: Modeling human preferences using transformers for rl.\" arXiv preprint arXiv:2303.00957 (2023).\n\n[3] Zhang, Zhilong, et al. \"Flow to better: Offline preference-based reinforcement learning via preferred trajectory generation.\" The Twelfth International Conference on Learning Representations. 2023.",
          "questions": "See the Weakness section.",
          "flag_for_ethics_review": "No ethics review needed.",
          "rating": 5,
          "confidence": 4,
          "code_of_conduct": "Yes"
        }
      ],
      "contribution": "In summary, our contributions are as follows. * We propose a general framework to bridge the gap between offline PBRL and standard offline RL. Our framework involves labeling the dataset with the reward that maintains most information in the preference signals. Afterward, one can train on the reward-labeled dataset with any standard offline RL algorithms. Our framework can be easily implemented without modifying the original RL algorithms. One can possibly construct more efficient PBRL algorithms by applying SOTA standard RL algorithms to our framework. * We mathematically analyze the combination of our framework with standard offline RL algorithms. We show that current state-of-the-art PBRL techniques are closely related to the combination of our framework and some specific standard offline RL algorithms regarding utilizing the preference labels. * We empirically show that our method performs significantly better on standard evaluation benchmarks than existing SOTA methods. In many cases, our method can even compete with training the same RL algorithm on the corresponding dataset of true reward labels.",
      "paper_id": "1Ffzgglq2I",
      "pdf": "/pdf/9d707d7297f2999fedb344ecb95199dc5acab694.pdf",
      "venue": "ICLR 2025 Conference Withdrawn Submission",
      "summary": {
        "introduction": "The introduction discusses the significance of offline reinforcement learning (RL) and the challenge of transferring insights from reward-based to preference-based RL (PBRL). It highlights the gap in existing methods, which mostly focus on reward feedback, and proposes a framework to bridge this gap by transforming preference feedback into scalar rewards through binary reward labeling (BRL). This allows the use of existing offline RL algorithms on preference datasets. The authors emphasize the importance of minimizing information loss during feedback transition, and explore theoretical connections between recent PBRL techniques and their framework. They empirically validate their approach on standard benchmarks, showing comparable or superior performance to state-of-the-art methods.",
        "related_work": "The related work section reviews existing offline reward-based RL, emphasizing the importance of addressing distribution mismatch and pessimism in offline learning. It discusses various algorithms, including model-based and model-free methods, and their theoretical foundations. For offline PBRL, it covers recent studies on theoretical guarantees, language model fine-tuning through RLHF, reward modeling, preference modeling, and active preference learning. Notably, it points out that most empirical work on offline PBRL is limited and does not encompass the general RL setting, highlighting the novelty and importance of their proposed framework.",
        "background": "The background section provides foundational concepts in reinforcement learning (RL), preference-based RL (PBRL), and the offline setting. It describes the standard RL framework with Markov Decision Processes, policies, and the goal of maximizing cumulative reward. It then introduces PBRL as a setting where feedback consists of preferences over trajectories rather than scalar rewards, modeled via a preference model with a link function relating preferences to rewards. The offline PBRL setting is detailed, where the dataset contains preference labels for trajectory pairs without direct access to environment rewards. The section emphasizes the goal of learning high-reward policies from preference datasets.",
        "method": "The method section introduces the core idea of transforming preference data into scalar rewards via reward labeling (BRL). It discusses how to evaluate the quality of reward labels by their ability to predict preferences, using a link function and loss measures. The optimal reward labels are those minimizing the prediction loss, and in the case of unique state-action pairs, the labels simplify to binary signals (maximal for favored trajectories, minimal for rejected ones). The section details an algorithm for applying any offline reward-based RL method to the reward-labeled data and provides a theoretical analysis showing the connection between their framework and existing PBRL techniques. It confirms that the binary labeling technique is optimal in certain practical scenarios and offers a simpler alternative to reward modeling.",
        "experiments": "The experiments involve constructing preference datasets from the D4RL benchmark by sampling trajectory pairs and generating synthetic preferences using a probabilistic model. They evaluate multiple offline RL algorithms, including model-based (MOPO, COMBO) and model-free (IQL, CQL) methods, applied to the reward-labeled datasets. The baseline comparisons include a reward modeling approach, an existing state-of-the-art PBRL method (IPL), and oracle performance with true rewards. The setup includes experiments without trajectory overlap—showing their method’s superior learning efficiency—and with trajectory overlaps, comparing reward labeling versus reward modeling. Additional ablation studies analyze performance over datasets of varying sizes and trajectory overlaps, demonstrating the robustness and efficiency of their reward labeling approach.",
        "results": "The results show that the proposed BRL framework generally outperforms baseline methods, including reward modeling and existing PBRL algorithms, especially in datasets without trajectory overlap. It achieves learning performance close to that of an oracle trained on true rewards, illustrating high efficiency. In cases of trajectory overlap, reward labeling via BRL remains more effective than reward modeling, as evidenced by larger gaps between reward labels and better policy performance. The ablation studies reveal that increasing preference dataset size improves performance of BRL significantly, while other methods show less variation. Overall, empirical results validate that BRL effectively bridges preference feedback to reward-based RL, leading to high-quality policies.",
        "discussion": "The discussion interprets the experimental findings, confirming that the binary reward labeling technique preserves essential information from preference signals more effectively than reward modeling, particularly in diverse scenarios with or without trajectory overlaps. It emphasizes that the method’s simplicity and lack of need for reward model training make it computationally advantageous. Theoretically, the analysis links their framework to existing PBRL approaches, showing conditions under which these methods are equivalent or closely related. Limitations include restriction to offline settings and assumptions about trajectory selection for labeling. The authors suggest that their approach offers a practical, easy-to-implement solution that leverages well-developed standard RL algorithms for preference-based tasks.",
        "conclusion": "The conclusion summarizes the main contribution: a general framework (BRL) that connects offline PBRL to standard offline RL by converting preferences into rewards via reward labeling. This allows existing RL algorithms to be directly applied, improving efficiency and performance. The authors highlight that their method achieves near-optimal results in benchmark tasks and outperforms recent PBRL baselines. Limitations include focus on the offline setting and assumptions about the selection of trajectories for labeling. They propose that future work could explore adaptive trajectory selection strategies and extend to online or active preference learning scenarios."
      }
    }
]